{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "install_depends = False\n",
    "download_data = False\n",
    "plot_dist = True # Only available when running locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Txilod5-gZ6F"
   },
   "source": [
    "Install python dev tools and dask dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7yvTmMoga4t",
    "outputId": "4126d683-4689-4c91-db57-b3b50f5441a1"
   },
   "outputs": [],
   "source": [
    "if install_depends:\n",
    "    !pip install colab-dev-tools\n",
    "    !python -m pip install \"dask[dataframe]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuDfJ71TXH5H"
   },
   "source": [
    "Get flight data tar from my drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lIfZf9X2ZHxH",
    "outputId": "906ddc2c-14e8-4c03-a209-945b977b904b"
   },
   "outputs": [],
   "source": [
    "if download_data:\n",
    "    !gdown '1Ms9cVOIsf-Thr_ZDPLyjkXufQaB7NBJQ'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMjErKy8ZS9G"
   },
   "source": [
    "extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8A-dgrxXZUs2"
   },
   "outputs": [],
   "source": [
    "if download_data:\n",
    "    import tarfile\n",
    "    _ = tarfile.open('all_flight.tar.gz').extractall('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGcIJFJm-9wQ"
   },
   "source": [
    "Clone the causal jazz repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S7TK-FRF_Azx",
    "outputId": "1edd67da-ac13-4073-adf7-4dde9f500aa9"
   },
   "outputs": [],
   "source": [
    "if install_depends:\n",
    "    !git clone https://github.com/hugh-osborne/causaljazz.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWjRymWDgd8d"
   },
   "source": [
    "Move to the repo home directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zx_KQLXggeX9",
    "outputId": "e7594c1f-51a5-4765-e890-dd755e417190"
   },
   "outputs": [],
   "source": [
    "if install_depends:\n",
    "    import os\n",
    "    os.chdir('causaljazz')\n",
    "    !git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNwZu5sDgf2e"
   },
   "source": [
    "Manually install causal jazz. At some point we'll publish to pypi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sxywh4gaghMt",
    "outputId": "a6949837-ea9c-407b-f799-c10cae056f27"
   },
   "outputs": [],
   "source": [
    "if install_depends:\n",
    "    !pip install .\n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21vlZRcy_KiQ"
   },
   "source": [
    "Run the example. Note, to begin with, set generate_models = True so that the model can be learned (later we can save the files.). Once the files have been generated, set it to false for repeat runs to save time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0_Gk91eyBwx"
   },
   "source": [
    "Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OecIvo2dyC9W",
    "outputId": "58d6d29c-c0b5-44a2-983c-9948718ebe11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.1 (SDL 2.28.2, Python 3.11.4)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "if plot_dist:\n",
    "    from causaljazz.visualiser import Visualiser\n",
    "from causaljazz.inference import TEDAG\n",
    "from causaljazz.inference import TEDAG_FUNCTION\n",
    "from causaljazz.cpu import pmf\n",
    "from causaljazz.cpu import transition\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUBopGg-vtee"
   },
   "source": [
    "Define some tensorflow model classes to be used later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "oEE7Jsexv-iH"
   },
   "outputs": [],
   "source": [
    "# Class to learn a constant variance (with constant input val[0]) between the expected value val[1] and the predicted value val[2]\n",
    "class NoiseModel(tf.keras.Model):\n",
    "    def __init__(self, inputs, var_model, **kwargs):\n",
    "        super().__init__(inputs, **kwargs)\n",
    "        self.var_model = var_model\n",
    "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            vals = []\n",
    "            for d in data[0]:\n",
    "                vals = vals + [d]\n",
    "\n",
    "            var_model_z = self.var_model([vals[0]])\n",
    "            loss = tf.reduce_sum(tf.keras.losses.mean_squared_error((vals[2]-vals[1])*(vals[2]-vals[1]), var_model_z))\n",
    "\n",
    "            total_loss = loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "# Autoencoder class to learn throttle - possibly this can be generalised?\n",
    "class ThrottleModel(tf.keras.Model):\n",
    "    def __init__(self, inputs, throttle_encoder, throttle_decoder, **kwargs):\n",
    "        super().__init__(inputs, **kwargs)\n",
    "        self.throttle_encoder = throttle_encoder\n",
    "        self.throttle_decoder = throttle_decoder\n",
    "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            vals = []\n",
    "            for d in data[0]:\n",
    "                vals = vals + [d]\n",
    "\n",
    "            throttle_encoder_z = self.throttle_encoder([vals[1], vals[2], vals[0], vals[3]])\n",
    "            throttle_decoder_z = self.throttle_decoder([throttle_encoder_z, vals[1], vals[2], vals[3]])\n",
    "            loss = tf.reduce_sum(tf.keras.losses.mean_squared_error(vals[0], throttle_decoder_z))\n",
    "\n",
    "            total_loss = loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "# class to learn the timescale 'a' of a derivative where dx/dt = a*(stationary - x)\n",
    "# The timescale is constant and has the constant input 1 (vals[3])\n",
    "# vals[0] : predicted stationary value\n",
    "# vals[1] : current actual or predicted value\n",
    "# vals[2] : actual dx/dt (in our case the diff between two timepoints)\n",
    "# vals[3] : constant ones\n",
    "class DerivModel(tf.keras.Model):\n",
    "    def __init__(self, inputs, ts_model, **kwargs):\n",
    "        super().__init__(inputs, **kwargs)\n",
    "        self.ts_model = ts_model\n",
    "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            vals = []\n",
    "            for d in data[0]:\n",
    "                vals = vals + [d]\n",
    "\n",
    "            ts_model_z  = self.ts_model([vals[3]])\n",
    "            loss = tf.reduce_sum(tf.keras.losses.mean_absolute_error(tf.abs(vals[2]),(tf.multiply(ts_model_z,tf.abs(vals[0] - vals[1])))))\n",
    "\n",
    "            total_loss = loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMmlppB5wnQW"
   },
   "source": [
    "Now some helper functions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "l9EMaU9nwpmW"
   },
   "outputs": [],
   "source": [
    "def build_pmf(sample, num_intervals = 100):\n",
    "  vmin = np.min(sample)\n",
    "  vmax = np.max(sample)\n",
    "\n",
    "  if vmax-vmin == 0:\n",
    "      vmin -= 0.005\n",
    "      vmax += 0.005\n",
    "\n",
    "  # Build a pmf for each latent variable\n",
    "  num_points = sample.shape[0]\n",
    "  mass_per_point = 1.0 / num_points\n",
    "  interval_width = (vmax-vmin)/num_intervals\n",
    "  pmf = np.zeros(num_intervals+1)\n",
    "  for i in sample:\n",
    "      idx = int((i - vmin) / interval_width)\n",
    "      pmf[idx] += mass_per_point\n",
    "\n",
    "  return pmf\n",
    "\n",
    "def build_cmf(pmf, num_intervals = 100):\n",
    "  # build cmfs for sampling\n",
    "  cmf = np.zeros(num_intervals+1)\n",
    "  mass = 0.0\n",
    "  for i in range(num_intervals+1):\n",
    "      mass += pmf[i]\n",
    "      cmf[i] = mass\n",
    "\n",
    "  return cmf\n",
    "\n",
    "def sample_from_cmf(cmf, num_points, vmin, vmax, num_intervals = 100):\n",
    "  interval_width = (vmax-vmin)/num_intervals\n",
    "\n",
    "  noise_rand = np.array([np.random.uniform() for a in range(num_points)])\n",
    "\n",
    "  noise_sampled = np.zeros(num_points)\n",
    "  for et in range(num_points):\n",
    "      for i in range(num_intervals+1):\n",
    "          if noise_rand[et] < cmf[i]:\n",
    "              noise_sampled[et] = vmin + (i * interval_width) + (interval_width * np.random.uniform())\n",
    "              break\n",
    "\n",
    "  return noise_sampled\n",
    "\n",
    "# get the data we care about for all flights for a given timestep\n",
    "def getFlightDataAtTimestep(dataset, current_timestep):\n",
    "  if current_timestep == 0:\n",
    "      current_timestep = 1\n",
    "\n",
    "  training_data = []\n",
    "\n",
    "  current_t = dataset[dataset['timestep'] == current_timestep]\n",
    "  current_t = current_t[['E1 RPM', 'E1 EGT1', 'E1 FFlow', 'VSpd', 'AltMSL', 'IAS', 'E1 OilP']]\n",
    "  previous_t = dataset[dataset['timestep'] == current_timestep-1]\n",
    "  previous_t = previous_t[['E1 RPM', 'E1 EGT1']]\n",
    "\n",
    "  # Loop through all flights up the max we want to view\n",
    "  flight_count = 0\n",
    "\n",
    "  for flight_index in [a for a in current_t.index]:\n",
    "      if flight_count > max_flights:\n",
    "          continue\n",
    "      flight_count += 1\n",
    "\n",
    "      if flight_index not in [a for a in current_t.index] or flight_index not in [a for a in previous_t.index]:\n",
    "          continue\n",
    "\n",
    "      vals_t = current_t.loc[flight_index].to_numpy().tolist()\n",
    "      vals_prev_t = previous_t.loc[flight_index].to_numpy().tolist()\n",
    "\n",
    "      # For the two variables we want, if either are NaN, just ignore this data point.\n",
    "      if (np.any([math.isnan(x) for x in vals_t]) or np.any([math.isnan(x) for x in vals_prev_t])):\n",
    "          continue\n",
    "\n",
    "      #                                  'E1 RPM', 'E1 EGT1', 'E1 FFlow',  'VSpd',   'AltMSL',   'IAS'             'dRPM',                       'dEGT'             'OilP'\n",
    "      training_data = training_data + [[vals_t[0], vals_t[1], vals_t[2], vals_t[3], vals_t[4], vals_t[5], vals_t[0] - vals_prev_t[0], vals_t[1] - vals_prev_t[1], vals_t[6]]]\n",
    "\n",
    "  return training_data\n",
    "\n",
    "def normaliseAndFormatData(data):\n",
    "  data = ((np.array(data)-min_vals) / max_vals) - 0.5\n",
    "  data_items = []\n",
    "  for d in range(data.shape[1]):\n",
    "      data_items = data_items + [np.reshape(data[:,d], [data[:,d].shape[0],1])]\n",
    "  return data_items\n",
    "\n",
    "# If we want to generate a pmf from a sample of points, we can use this function to get the desired cell widths\n",
    "def calculateCellGridMetrics(data, res):\n",
    "    in_mins = np.min(data, axis=1)\n",
    "    in_maxs = np.max(data, axis=1)\n",
    "    in_ranges = in_maxs - in_mins\n",
    "    in_ranges = np.array([r if r > 0 else 0.00001 for r in in_ranges]) # make sure no range is zero and give it a small epsilon range if so\n",
    "    return in_mins, in_maxs, in_ranges, np.divide(in_ranges,res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zjWPqKPwGvw"
   },
   "source": [
    "Now load the data. This takes some time but luckily only needs to be called once then later processing can be repeated - the magic of iPython!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mRw9H8OawR1A"
   },
   "outputs": [],
   "source": [
    "observed_variables = ['E1 RPM', 'IAS', 'E1 FFlow', 'E1 EGT1', 'AltMSL', 'VSpd', 'E1 OilP'] # In the future, add all four EGTs?\n",
    "\n",
    "# Define a maximum number of timesteps and flights to load\n",
    "max_flights = 500\n",
    "max_num_timesteps = 200\n",
    "\n",
    "# Load the flight header info\n",
    "flight_header_df = pd.read_csv('all_flights/flight_header.csv', index_col='Master Index')\n",
    "# Optionally search only for flights corresponding to a specific maintenance label\n",
    "flight_header_df = flight_header_df[flight_header_df['label'].str.contains('oil')]\n",
    "\n",
    "# Split flights into those before maintenance and after maintenance\n",
    "after_flights = flight_header_df[flight_header_df['number_flights_before'] == -1].index # -1 == flight after\n",
    "before_flights = flight_header_df[flight_header_df['number_flights_before'] == 0].index # >=0 == flight before\n",
    "\n",
    "# Load the flight data - to speed things up, only select the two (or more) variabels we care about olus timestep\n",
    "all_interested_columns = observed_variables + ['timestep']\n",
    "flight_data_ddf = dd.read_parquet('all_flights/one_parq', columns=all_interested_columns, index_col='Master Index')\n",
    "\n",
    "# Using the header data, split the flight data into before and after maintenance flights\n",
    "# Only read timesteps below the max required\n",
    "indices = [a for a in after_flights]\n",
    "flight_data_ddf_after = flight_data_ddf.loc[flight_data_ddf.index.isin(indices)]\n",
    "flight_data_ddf_after = flight_data_ddf_after.loc[flight_data_ddf_after['timestep'] < max_num_timesteps]\n",
    "flight_data_ddf_after = flight_data_ddf_after.loc[flight_data_ddf_after['AltMSL'] > 200] # avoid planes at ground level\n",
    "flight_data_df_after = flight_data_ddf_after.compute()\n",
    "\n",
    "indices = [a for a in before_flights]\n",
    "flight_data_ddf_before = flight_data_ddf.loc[flight_data_ddf.index.isin(indices)]\n",
    "flight_data_ddf_before = flight_data_ddf_before.loc[flight_data_ddf_before['timestep'] < max_num_timesteps]\n",
    "flight_data_ddf_before = flight_data_ddf_before.loc[flight_data_ddf_before['AltMSL'] > 200]\n",
    "flight_data_df_before = flight_data_ddf_before.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsCQkKqrxMzI"
   },
   "source": [
    "Format the data into a nice structure. See getFlightDataAtTimestep for the order of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Ao-KUGCOxS23"
   },
   "outputs": [],
   "source": [
    "training_data_after = []\n",
    "# load training data for flights after maintenance\n",
    "for current_timestep in [a+1 for a in range(max_num_timesteps-1)]:\n",
    "    training_data_after = training_data_after + getFlightDataAtTimestep(flight_data_df_after, current_timestep)\n",
    "\n",
    "min_vals = np.min(training_data_after, axis=0)\n",
    "max_vals  = np.max(training_data_after, axis=0)\n",
    "training_data_after = ((np.array(training_data_after)-min_vals) / max_vals) - 0.5\n",
    "\n",
    "training_data_before = []\n",
    "# load training data for flights before maintenance\n",
    "for current_timestep in [a+1 for a in range(max_num_timesteps-1)]:\n",
    "    training_data_before = training_data_before + getFlightDataAtTimestep(flight_data_df_before, current_timestep)\n",
    "\n",
    "training_data_before = ((np.array(training_data_before)-min_vals) / max_vals) - 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTE2RxDsxsro"
   },
   "source": [
    "Now define the different tensorflow models and train them if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JE-t6F9-xyEP",
    "outputId": "0825963b-e180-41c0-cda9-c84387e3da27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"throttle_enc\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_10 (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_11 (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 4)                    0         ['input_10[0][0]',            \n",
      "                                                                     'input_11[0][0]',            \n",
      "                                                                     'input_4[0][0]',             \n",
      "                                                                     'input_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 200)                  1000      ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 200)                  40200     ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 1)                    201       ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 41401 (161.72 KB)\n",
      "Trainable params: 41401 (161.72 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"throttle_dec\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_11 (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 4)                    0         ['input_13[0][0]',            \n",
      " )                                                                   'input_10[0][0]',            \n",
      "                                                                     'input_11[0][0]',            \n",
      "                                                                     'input_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 200)                  1000      ['concatenate_1[1][0]']       \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 200)                  40200     ['dense_3[1][0]']             \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 1)                    201       ['dense_4[1][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 41401 (161.72 KB)\n",
      "Trainable params: 41401 (161.72 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"egt_stat\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 2)                    0         ['input_4[0][0]',             \n",
      " )                                                                   'input_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 200)                  600       ['concatenate_2[0][0]']       \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 200)                  40200     ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 1)                    201       ['dense_7[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 41001 (160.16 KB)\n",
      "Trainable params: 41001 (160.16 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"egt_ts\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " concatenate_3 (Concatenate  (None, 1)                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2 (8.00 Byte)\n",
      "Trainable params: 2 (8.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"egt_stat_var\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " concatenate_4 (Concatenate  (None, 1)                 0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " )                                                               \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2 (8.00 Byte)\n",
      "Trainable params: 2 (8.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"oilp\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 20)                40        \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61 (244.00 Byte)\n",
      "Trainable params: 61 (244.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"oilp_var\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " concatenate_5 (Concatenate  (None, 1)                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2 (8.00 Byte)\n",
      "Trainable params: 2 (8.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Throttle/FFlow Training...\n",
      "Epoch 1/200\n",
      "57/57 [==============================] - 1s 4ms/step - loss: 13.2420\n",
      "Epoch 2/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0286\n",
      "Epoch 3/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 3.5702e-04\n",
      "Epoch 4/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.1043\n",
      "Epoch 5/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0194\n",
      "Epoch 6/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 9.4705e-04\n",
      "Epoch 7/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0648\n",
      "Epoch 8/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0435\n",
      "Epoch 9/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0275\n",
      "Epoch 10/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.5279\n",
      "Epoch 11/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 0.0034\n",
      "Epoch 12/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 5.3725e-05\n",
      "Epoch 13/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 1.3512e-06\n",
      "Epoch 14/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 2.6287e-08\n",
      "Epoch 15/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 2.7894e-09\n",
      "Epoch 16/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 8.0341e-12\n",
      "Epoch 17/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 1.2848e-12\n",
      "Epoch 18/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 1.2730e-12\n",
      "Epoch 19/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 1.2620e-12\n",
      "Epoch 20/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 1.2772e-12\n",
      "Epoch 21/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 1.2643e-12\n",
      "Epoch 22/200\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.2803e-12\n",
      "Epoch 23/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 1.2810e-12\n",
      "Epoch 24/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 1.2707e-12\n",
      "Epoch 25/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 1.2826e-12\n",
      "Epoch 26/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 1.2899e-12\n",
      "Epoch 27/200\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.2583e-12\n",
      "Epoch 28/200\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.2755e-12\n",
      "Epoch 29/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 1.2630e-12\n",
      "Epoch 30/200\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.2728e-12\n",
      "Epoch 31/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 1.2797e-12\n",
      "Epoch 32/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 1.2727e-12\n",
      "Epoch 33/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 1.2605e-12\n",
      "Epoch 34/200\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.2732e-12\n",
      "Epoch 35/200\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.2768e-12\n",
      "Epoch 36/200\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.2802e-12\n",
      "Epoch 37/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 1.2606e-12\n",
      "Epoch 38/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 1.2693e-12\n",
      "Epoch 39/200\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 1.2462e-12\n",
      "Epoch 40/200\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.2815e-12\n",
      "OilP Training...\n",
      "Epoch 1/200\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.1185 - val_loss: 0.0816\n",
      "Epoch 2/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0672 - val_loss: 0.0469\n",
      "Epoch 3/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0462 - val_loss: 0.0381\n",
      "Epoch 4/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0431 - val_loss: 0.0390\n",
      "Epoch 5/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0430 - val_loss: 0.0390\n",
      "Epoch 6/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0430 - val_loss: 0.0390\n",
      "Epoch 7/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0430 - val_loss: 0.0389\n",
      "Epoch 8/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0431 - val_loss: 0.0392\n",
      "Epoch 9/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0431 - val_loss: 0.0389\n",
      "Epoch 10/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0430 - val_loss: 0.0387\n",
      "Epoch 11/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0430 - val_loss: 0.0391\n",
      "Epoch 12/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0431 - val_loss: 0.0388\n",
      "Epoch 13/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0431 - val_loss: 0.0392\n",
      "Epoch 14/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0431 - val_loss: 0.0389\n",
      "Epoch 15/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0431 - val_loss: 0.0392\n",
      "Epoch 16/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0431 - val_loss: 0.0393\n",
      "Epoch 17/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0431 - val_loss: 0.0389\n",
      "Epoch 18/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0431 - val_loss: 0.0388\n",
      "Epoch 19/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0431 - val_loss: 0.0392\n",
      "Epoch 20/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0431 - val_loss: 0.0394\n",
      "Epoch 21/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0431 - val_loss: 0.0388\n",
      "Epoch 22/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0431 - val_loss: 0.0390\n",
      "Epoch 23/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0431 - val_loss: 0.0388\n",
      "Epoch 24/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0431 - val_loss: 0.0390\n",
      "Epoch 25/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0431 - val_loss: 0.0389\n",
      "INFO:tensorflow:Assets written to: oilp.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: oilp.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EGT Stationary Training...\n",
      "Epoch 1/200\n",
      "46/46 [==============================] - 1s 5ms/step - loss: 0.0645 - val_loss: 0.0460\n",
      "Epoch 2/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.0418\n",
      "Epoch 3/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.0410\n",
      "Epoch 4/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.0418\n",
      "Epoch 5/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0437 - val_loss: 0.0421\n",
      "Epoch 6/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.0417\n",
      "Epoch 7/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.0411\n",
      "Epoch 8/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.0426\n",
      "Epoch 9/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0440 - val_loss: 0.0424\n",
      "Epoch 10/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.0432\n",
      "Epoch 11/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0437 - val_loss: 0.0438\n",
      "Epoch 12/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.0413\n",
      "Epoch 13/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.0430\n",
      "Epoch 14/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0437 - val_loss: 0.0434\n",
      "Epoch 15/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.0516\n",
      "Epoch 16/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0445 - val_loss: 0.0412\n",
      "Epoch 17/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.0460\n",
      "Epoch 18/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0440 - val_loss: 0.0420\n",
      "Epoch 19/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0437 - val_loss: 0.0429\n",
      "Epoch 20/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.0423\n",
      "Epoch 21/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0440 - val_loss: 0.0413\n",
      "Epoch 22/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0446 - val_loss: 0.0416\n",
      "Epoch 23/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0437 - val_loss: 0.0425\n",
      "Epoch 24/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.0417\n",
      "Epoch 25/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.0422\n",
      "Epoch 26/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.0434\n",
      "Epoch 27/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.0425\n",
      "Epoch 28/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.0442\n",
      "Epoch 29/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.0438\n",
      "Epoch 30/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0437 - val_loss: 0.0423\n",
      "Epoch 31/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.0438\n",
      "Epoch 32/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.0439\n",
      "Epoch 33/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.0424\n",
      "Epoch 34/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.0420\n",
      "Epoch 35/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.0422\n",
      "Epoch 36/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.0430\n",
      "Epoch 37/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.0419\n",
      "Epoch 38/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0440 - val_loss: 0.0420\n",
      "Epoch 39/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0437 - val_loss: 0.0414\n",
      "Epoch 40/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0440 - val_loss: 0.0447\n",
      "Epoch 41/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.0432\n",
      "Epoch 42/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0440 - val_loss: 0.0428\n",
      "Epoch 43/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0437 - val_loss: 0.0424\n",
      "INFO:tensorflow:Assets written to: egt_stat.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: egt_stat.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timescale Training...\n",
      "Epoch 1/200\n",
      "57/57 [==============================] - 0s 950us/step - loss: 1079.8878\n",
      "Epoch 2/200\n",
      "57/57 [==============================] - 0s 897us/step - loss: 1078.9506\n",
      "Epoch 3/200\n",
      "57/57 [==============================] - 0s 965us/step - loss: 1077.6598\n",
      "Epoch 4/200\n",
      "57/57 [==============================] - 0s 946us/step - loss: 1076.5483\n",
      "Epoch 5/200\n",
      "57/57 [==============================] - 0s 911us/step - loss: 1075.0256\n",
      "Epoch 6/200\n",
      "57/57 [==============================] - 0s 949us/step - loss: 1073.9747\n",
      "Epoch 7/200\n",
      "57/57 [==============================] - 0s 903us/step - loss: 1072.8111\n",
      "Epoch 8/200\n",
      "57/57 [==============================] - 0s 920us/step - loss: 1071.6598\n",
      "Epoch 9/200\n",
      "57/57 [==============================] - 0s 913us/step - loss: 1070.5010\n",
      "Epoch 10/200\n",
      "57/57 [==============================] - 0s 974us/step - loss: 1069.5577\n",
      "Epoch 11/200\n",
      "57/57 [==============================] - 0s 915us/step - loss: 1068.6149\n",
      "Epoch 12/200\n",
      "57/57 [==============================] - 0s 891us/step - loss: 1067.3868\n",
      "Epoch 13/200\n",
      "57/57 [==============================] - 0s 909us/step - loss: 1066.8171\n",
      "Epoch 14/200\n",
      "57/57 [==============================] - 0s 937us/step - loss: 1066.3051\n",
      "Epoch 15/200\n",
      "57/57 [==============================] - 0s 948us/step - loss: 1065.1105\n",
      "Epoch 16/200\n",
      "57/57 [==============================] - 0s 961us/step - loss: 1064.3720\n",
      "Epoch 17/200\n",
      "57/57 [==============================] - 0s 913us/step - loss: 1064.1302\n",
      "Epoch 18/200\n",
      "57/57 [==============================] - 0s 907us/step - loss: 1063.5669\n",
      "Epoch 19/200\n",
      "57/57 [==============================] - 0s 946us/step - loss: 1063.0071\n",
      "Epoch 20/200\n",
      "57/57 [==============================] - 0s 908us/step - loss: 1062.6775\n",
      "Epoch 21/200\n",
      "57/57 [==============================] - 0s 910us/step - loss: 1062.3100\n",
      "Epoch 22/200\n",
      "57/57 [==============================] - 0s 915us/step - loss: 1062.1164\n",
      "Epoch 23/200\n",
      "57/57 [==============================] - 0s 924us/step - loss: 1061.1512\n",
      "Epoch 24/200\n",
      "57/57 [==============================] - 0s 991us/step - loss: 1061.2859\n",
      "Epoch 25/200\n",
      "57/57 [==============================] - 0s 904us/step - loss: 1060.8234\n",
      "Epoch 26/200\n",
      "57/57 [==============================] - 0s 928us/step - loss: 1060.7299\n",
      "Epoch 27/200\n",
      "57/57 [==============================] - 0s 883us/step - loss: 1060.2227\n",
      "Epoch 28/200\n",
      "57/57 [==============================] - 0s 906us/step - loss: 1059.9505\n",
      "Epoch 29/200\n",
      "57/57 [==============================] - 0s 917us/step - loss: 1059.5641\n",
      "Epoch 30/200\n",
      "57/57 [==============================] - 0s 952us/step - loss: 1059.0776\n",
      "Epoch 31/200\n",
      "57/57 [==============================] - 0s 943us/step - loss: 1059.3581\n",
      "Epoch 32/200\n",
      "57/57 [==============================] - 0s 902us/step - loss: 1059.1188\n",
      "Epoch 33/200\n",
      "57/57 [==============================] - 0s 974us/step - loss: 1058.9484\n",
      "Epoch 34/200\n",
      "57/57 [==============================] - 0s 910us/step - loss: 1058.7649\n",
      "Epoch 35/200\n",
      "57/57 [==============================] - 0s 917us/step - loss: 1058.7932\n",
      "Epoch 36/200\n",
      "57/57 [==============================] - 0s 953us/step - loss: 1058.4454\n",
      "Epoch 37/200\n",
      "57/57 [==============================] - 0s 927us/step - loss: 1058.6522\n",
      "Epoch 38/200\n",
      "57/57 [==============================] - 0s 913us/step - loss: 1058.1856\n",
      "Epoch 39/200\n",
      "57/57 [==============================] - 0s 947us/step - loss: 1058.3990\n",
      "Epoch 40/200\n",
      "57/57 [==============================] - 0s 965us/step - loss: 1057.6187\n",
      "Epoch 41/200\n",
      "57/57 [==============================] - 0s 915us/step - loss: 1058.2158\n",
      "Epoch 42/200\n",
      "57/57 [==============================] - 0s 976us/step - loss: 1057.7798\n",
      "Epoch 43/200\n",
      "57/57 [==============================] - 0s 948us/step - loss: 1057.7550\n",
      "Epoch 44/200\n",
      "57/57 [==============================] - 0s 935us/step - loss: 1057.4705\n",
      "Epoch 45/200\n",
      "57/57 [==============================] - 0s 928us/step - loss: 1057.4135\n",
      "Epoch 46/200\n",
      "57/57 [==============================] - 0s 903us/step - loss: 1057.5444\n",
      "Epoch 47/200\n",
      "57/57 [==============================] - 0s 937us/step - loss: 1057.5260\n",
      "Epoch 48/200\n",
      "57/57 [==============================] - 0s 968us/step - loss: 1057.6714\n",
      "Epoch 49/200\n",
      "57/57 [==============================] - 0s 904us/step - loss: 1057.7589\n",
      "Epoch 50/200\n",
      "57/57 [==============================] - 0s 925us/step - loss: 1057.1780\n",
      "Epoch 51/200\n",
      "57/57 [==============================] - 0s 903us/step - loss: 1057.1374\n",
      "Epoch 52/200\n",
      "57/57 [==============================] - 0s 957us/step - loss: 1057.1762\n",
      "Epoch 53/200\n",
      "57/57 [==============================] - 0s 923us/step - loss: 1056.5641\n",
      "Epoch 54/200\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 1056.4607\n",
      "Epoch 55/200\n",
      "57/57 [==============================] - 0s 912us/step - loss: 1056.9154\n",
      "Epoch 56/200\n",
      "57/57 [==============================] - 0s 894us/step - loss: 1057.1160\n",
      "Epoch 57/200\n",
      "57/57 [==============================] - 0s 909us/step - loss: 1056.7072\n",
      "Epoch 58/200\n",
      "57/57 [==============================] - 0s 920us/step - loss: 1056.7530\n",
      "Epoch 59/200\n",
      "57/57 [==============================] - 0s 910us/step - loss: 1056.3854\n",
      "Epoch 60/200\n",
      "57/57 [==============================] - 0s 917us/step - loss: 1057.0511\n",
      "Epoch 61/200\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 1056.4990\n",
      "Epoch 62/200\n",
      "57/57 [==============================] - 0s 913us/step - loss: 1056.5524\n",
      "Epoch 63/200\n",
      "57/57 [==============================] - 0s 929us/step - loss: 1056.6228\n",
      "Epoch 64/200\n",
      "57/57 [==============================] - 0s 895us/step - loss: 1056.2393\n",
      "Epoch 65/200\n",
      "57/57 [==============================] - 0s 910us/step - loss: 1056.1157\n",
      "Epoch 66/200\n",
      "57/57 [==============================] - 0s 907us/step - loss: 1056.1270\n",
      "Epoch 67/200\n",
      "57/57 [==============================] - 0s 950us/step - loss: 1056.4423\n",
      "Epoch 68/200\n",
      "57/57 [==============================] - 0s 913us/step - loss: 1056.4404\n",
      "Epoch 69/200\n",
      "57/57 [==============================] - 0s 918us/step - loss: 1056.5244\n",
      "Epoch 70/200\n",
      "57/57 [==============================] - 0s 955us/step - loss: 1056.1775\n",
      "Epoch 71/200\n",
      "57/57 [==============================] - 0s 940us/step - loss: 1056.3646\n",
      "Epoch 72/200\n",
      "57/57 [==============================] - 0s 925us/step - loss: 1056.4731\n",
      "Epoch 73/200\n",
      "57/57 [==============================] - 0s 934us/step - loss: 1056.6795\n",
      "Epoch 74/200\n",
      "57/57 [==============================] - 0s 954us/step - loss: 1056.3445\n",
      "Epoch 75/200\n",
      "57/57 [==============================] - 0s 969us/step - loss: 1056.0006\n",
      "Epoch 76/200\n",
      "57/57 [==============================] - 0s 969us/step - loss: 1055.8339\n",
      "Epoch 77/200\n",
      "57/57 [==============================] - 0s 952us/step - loss: 1055.8654\n",
      "Epoch 78/200\n",
      "57/57 [==============================] - 0s 996us/step - loss: 1055.9989\n",
      "Epoch 79/200\n",
      "57/57 [==============================] - 0s 962us/step - loss: 1056.1887\n",
      "Epoch 80/200\n",
      "57/57 [==============================] - 0s 955us/step - loss: 1056.0248\n",
      "Epoch 81/200\n",
      "57/57 [==============================] - 0s 965us/step - loss: 1055.8603\n",
      "Epoch 82/200\n",
      "57/57 [==============================] - 0s 920us/step - loss: 1056.2183\n",
      "Epoch 83/200\n",
      "57/57 [==============================] - 0s 975us/step - loss: 1055.7751\n",
      "Epoch 84/200\n",
      "57/57 [==============================] - 0s 927us/step - loss: 1056.2156\n",
      "Epoch 85/200\n",
      "57/57 [==============================] - 0s 958us/step - loss: 1055.8292\n",
      "Epoch 86/200\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 1055.9295\n",
      "Epoch 87/200\n",
      "57/57 [==============================] - 0s 913us/step - loss: 1056.1571\n",
      "Epoch 88/200\n",
      "57/57 [==============================] - 0s 936us/step - loss: 1056.5392\n",
      "Epoch 89/200\n",
      "57/57 [==============================] - 0s 956us/step - loss: 1055.9654\n",
      "Epoch 90/200\n",
      "57/57 [==============================] - 0s 908us/step - loss: 1056.1158\n",
      "Epoch 91/200\n",
      "57/57 [==============================] - 0s 965us/step - loss: 1055.9921\n",
      "Epoch 92/200\n",
      "57/57 [==============================] - 0s 925us/step - loss: 1056.4167\n",
      "Epoch 93/200\n",
      "57/57 [==============================] - 0s 959us/step - loss: 1055.8908\n",
      "Epoch 94/200\n",
      "57/57 [==============================] - 0s 983us/step - loss: 1055.8346\n",
      "Epoch 95/200\n",
      "57/57 [==============================] - 0s 930us/step - loss: 1055.6370\n",
      "Epoch 96/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 938us/step - loss: 1055.9127\n",
      "Epoch 97/200\n",
      "57/57 [==============================] - 0s 985us/step - loss: 1055.7102\n",
      "Epoch 98/200\n",
      "57/57 [==============================] - 0s 953us/step - loss: 1055.6164\n",
      "Epoch 99/200\n",
      "57/57 [==============================] - 0s 951us/step - loss: 1055.6720\n",
      "Epoch 100/200\n",
      "57/57 [==============================] - 0s 915us/step - loss: 1056.3576\n",
      "Epoch 101/200\n",
      "57/57 [==============================] - 0s 972us/step - loss: 1056.1138\n",
      "Epoch 102/200\n",
      "57/57 [==============================] - 0s 930us/step - loss: 1055.7255\n",
      "Epoch 103/200\n",
      "57/57 [==============================] - 0s 906us/step - loss: 1055.8163\n",
      "Epoch 104/200\n",
      "57/57 [==============================] - 0s 950us/step - loss: 1055.6811\n",
      "Epoch 105/200\n",
      "57/57 [==============================] - 0s 965us/step - loss: 1056.3964\n",
      "Epoch 106/200\n",
      "57/57 [==============================] - 0s 921us/step - loss: 1055.8485\n",
      "Epoch 107/200\n",
      "57/57 [==============================] - 0s 924us/step - loss: 1055.6160\n",
      "Epoch 108/200\n",
      "57/57 [==============================] - 0s 957us/step - loss: 1055.7902\n",
      "Epoch 109/200\n",
      "57/57 [==============================] - 0s 915us/step - loss: 1055.8046\n",
      "Epoch 110/200\n",
      "57/57 [==============================] - 0s 932us/step - loss: 1056.0431\n",
      "Epoch 111/200\n",
      "57/57 [==============================] - 0s 919us/step - loss: 1055.8482\n",
      "Epoch 112/200\n",
      "57/57 [==============================] - 0s 955us/step - loss: 1056.1406\n",
      "Epoch 113/200\n",
      "57/57 [==============================] - 0s 900us/step - loss: 1055.3194\n",
      "Epoch 114/200\n",
      "57/57 [==============================] - 0s 924us/step - loss: 1056.3449\n",
      "Epoch 115/200\n",
      "57/57 [==============================] - 0s 937us/step - loss: 1056.0961\n",
      "Epoch 116/200\n",
      "57/57 [==============================] - 0s 946us/step - loss: 1055.7774\n",
      "Epoch 117/200\n",
      "57/57 [==============================] - 0s 998us/step - loss: 1055.9518\n",
      "Epoch 118/200\n",
      "57/57 [==============================] - 0s 893us/step - loss: 1055.1534\n",
      "Epoch 119/200\n",
      "57/57 [==============================] - 0s 933us/step - loss: 1055.9313\n",
      "Epoch 120/200\n",
      "57/57 [==============================] - 0s 915us/step - loss: 1055.2961\n",
      "Epoch 121/200\n",
      "57/57 [==============================] - 0s 915us/step - loss: 1056.0968\n",
      "Epoch 122/200\n",
      "57/57 [==============================] - 0s 924us/step - loss: 1055.8109\n",
      "Epoch 123/200\n",
      "57/57 [==============================] - 0s 931us/step - loss: 1055.3735\n",
      "Epoch 124/200\n",
      "57/57 [==============================] - 0s 957us/step - loss: 1056.0787\n",
      "Epoch 125/200\n",
      "57/57 [==============================] - 0s 949us/step - loss: 1055.6965\n",
      "Epoch 126/200\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 1055.6162\n",
      "Epoch 127/200\n",
      "57/57 [==============================] - 0s 936us/step - loss: 1055.5444\n",
      "Epoch 128/200\n",
      "57/57 [==============================] - 0s 912us/step - loss: 1055.6586\n",
      "Epoch 129/200\n",
      "57/57 [==============================] - 0s 933us/step - loss: 1055.6061\n",
      "Epoch 130/200\n",
      "57/57 [==============================] - 0s 949us/step - loss: 1055.6360\n",
      "Epoch 131/200\n",
      "57/57 [==============================] - 0s 928us/step - loss: 1055.9969\n",
      "Epoch 132/200\n",
      "57/57 [==============================] - 0s 937us/step - loss: 1055.3479\n",
      "Epoch 133/200\n",
      "57/57 [==============================] - 0s 965us/step - loss: 1055.8662\n",
      "Epoch 134/200\n",
      "57/57 [==============================] - 0s 917us/step - loss: 1056.0076\n",
      "Epoch 135/200\n",
      "57/57 [==============================] - 0s 969us/step - loss: 1055.3721\n",
      "Epoch 136/200\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 1055.5086\n",
      "Epoch 137/200\n",
      "57/57 [==============================] - 0s 966us/step - loss: 1055.5759\n",
      "Epoch 138/200\n",
      "57/57 [==============================] - 0s 912us/step - loss: 1056.3111\n",
      "Epoch 139/200\n",
      "57/57 [==============================] - 0s 911us/step - loss: 1055.5171\n",
      "Epoch 140/200\n",
      "57/57 [==============================] - 0s 944us/step - loss: 1055.5373\n",
      "Epoch 141/200\n",
      "57/57 [==============================] - 0s 914us/step - loss: 1055.7773\n",
      "Epoch 142/200\n",
      "57/57 [==============================] - 0s 921us/step - loss: 1055.8598\n",
      "Epoch 143/200\n",
      "57/57 [==============================] - 0s 950us/step - loss: 1055.4711\n",
      "Epoch 144/200\n",
      "57/57 [==============================] - 0s 967us/step - loss: 1055.3824\n",
      "Epoch 145/200\n",
      "57/57 [==============================] - 0s 927us/step - loss: 1055.4852\n",
      "Epoch 146/200\n",
      "57/57 [==============================] - 0s 972us/step - loss: 1055.7367\n",
      "Epoch 147/200\n",
      "57/57 [==============================] - 0s 914us/step - loss: 1055.6381\n",
      "Epoch 148/200\n",
      "57/57 [==============================] - 0s 950us/step - loss: 1055.4316\n",
      "Epoch 149/200\n",
      "57/57 [==============================] - 0s 953us/step - loss: 1056.0312\n",
      "Epoch 150/200\n",
      "57/57 [==============================] - 0s 915us/step - loss: 1055.7287\n",
      "Epoch 151/200\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 1055.6000\n",
      "Epoch 152/200\n",
      "57/57 [==============================] - 0s 922us/step - loss: 1055.4212\n",
      "Epoch 153/200\n",
      "57/57 [==============================] - 0s 949us/step - loss: 1055.6470\n",
      "Epoch 154/200\n",
      "57/57 [==============================] - 0s 939us/step - loss: 1055.8420\n",
      "Epoch 155/200\n",
      "57/57 [==============================] - 0s 912us/step - loss: 1055.6215\n",
      "Epoch 156/200\n",
      "57/57 [==============================] - 0s 921us/step - loss: 1055.8264\n",
      "Epoch 157/200\n",
      "57/57 [==============================] - 0s 917us/step - loss: 1055.6333\n",
      "Epoch 158/200\n",
      "57/57 [==============================] - 0s 911us/step - loss: 1055.4838\n",
      "Epoch 159/200\n",
      "57/57 [==============================] - 0s 921us/step - loss: 1055.6425\n",
      "Epoch 160/200\n",
      "57/57 [==============================] - 0s 904us/step - loss: 1055.6183\n",
      "Epoch 161/200\n",
      "57/57 [==============================] - 0s 963us/step - loss: 1055.5726\n",
      "Epoch 162/200\n",
      "57/57 [==============================] - 0s 903us/step - loss: 1055.2109\n",
      "Epoch 163/200\n",
      "57/57 [==============================] - 0s 926us/step - loss: 1055.5676\n",
      "Epoch 164/200\n",
      "57/57 [==============================] - 0s 950us/step - loss: 1055.3012\n",
      "Epoch 165/200\n",
      "57/57 [==============================] - 0s 893us/step - loss: 1055.6695\n",
      "Epoch 166/200\n",
      "57/57 [==============================] - 0s 935us/step - loss: 1055.4296\n",
      "Epoch 167/200\n",
      "57/57 [==============================] - 0s 950us/step - loss: 1056.1752\n",
      "Epoch 168/200\n",
      "57/57 [==============================] - 0s 906us/step - loss: 1055.2839\n",
      "Epoch 169/200\n",
      "57/57 [==============================] - 0s 892us/step - loss: 1055.2694\n",
      "Epoch 170/200\n",
      "57/57 [==============================] - 0s 995us/step - loss: 1055.2678\n",
      "Epoch 171/200\n",
      "57/57 [==============================] - 0s 950us/step - loss: 1055.5652\n",
      "Epoch 172/200\n",
      "57/57 [==============================] - 0s 918us/step - loss: 1056.1448\n",
      "Epoch 173/200\n",
      "57/57 [==============================] - 0s 927us/step - loss: 1055.6257\n",
      "Epoch 174/200\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 1055.8401\n",
      "Epoch 175/200\n",
      "57/57 [==============================] - 0s 928us/step - loss: 1055.7074\n",
      "Epoch 176/200\n",
      "57/57 [==============================] - 0s 947us/step - loss: 1055.3990\n",
      "Epoch 177/200\n",
      "57/57 [==============================] - 0s 919us/step - loss: 1055.4559\n",
      "Epoch 178/200\n",
      "57/57 [==============================] - 0s 902us/step - loss: 1055.4092\n",
      "Epoch 179/200\n",
      "57/57 [==============================] - 0s 961us/step - loss: 1055.4881\n",
      "Epoch 180/200\n",
      "57/57 [==============================] - 0s 954us/step - loss: 1055.2279\n",
      "Epoch 181/200\n",
      "57/57 [==============================] - 0s 930us/step - loss: 1055.5901\n",
      "Epoch 182/200\n",
      "57/57 [==============================] - 0s 980us/step - loss: 1055.6958\n",
      "Epoch 183/200\n",
      "57/57 [==============================] - 0s 918us/step - loss: 1055.2682\n",
      "Epoch 184/200\n",
      "57/57 [==============================] - 0s 927us/step - loss: 1055.4081\n",
      "Epoch 185/200\n",
      "57/57 [==============================] - 0s 926us/step - loss: 1055.1960\n",
      "Epoch 186/200\n",
      "57/57 [==============================] - 0s 908us/step - loss: 1055.9039\n",
      "Epoch 187/200\n",
      "57/57 [==============================] - 0s 913us/step - loss: 1055.8174\n",
      "Epoch 188/200\n",
      "57/57 [==============================] - 0s 977us/step - loss: 1055.2074\n",
      "Epoch 189/200\n",
      "57/57 [==============================] - 0s 948us/step - loss: 1055.4080\n",
      "Epoch 190/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 936us/step - loss: 1055.8244\n",
      "Epoch 191/200\n",
      "57/57 [==============================] - 0s 955us/step - loss: 1055.5200\n",
      "Epoch 192/200\n",
      "57/57 [==============================] - 0s 898us/step - loss: 1055.6619\n",
      "Epoch 193/200\n",
      "57/57 [==============================] - 0s 916us/step - loss: 1055.6448\n",
      "Epoch 194/200\n",
      "57/57 [==============================] - 0s 946us/step - loss: 1055.4851\n",
      "Epoch 195/200\n",
      "57/57 [==============================] - 0s 924us/step - loss: 1055.7622\n",
      "Epoch 196/200\n",
      "57/57 [==============================] - 0s 896us/step - loss: 1055.4176\n",
      "Epoch 197/200\n",
      "57/57 [==============================] - 0s 914us/step - loss: 1055.5694\n",
      "Epoch 198/200\n",
      "57/57 [==============================] - 0s 927us/step - loss: 1055.9258\n",
      "Epoch 199/200\n",
      "57/57 [==============================] - 0s 959us/step - loss: 1055.8729\n",
      "Epoch 200/200\n",
      "57/57 [==============================] - 0s 908us/step - loss: 1055.4253\n",
      "EGT Noise Training...\n",
      "Epoch 1/200\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 1429.8544\n",
      "Epoch 2/200\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 1177.4053\n",
      "Epoch 3/200\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 959.5353\n",
      "Epoch 4/200\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 773.3348\n",
      "Epoch 5/200\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 617.0448\n",
      "Epoch 6/200\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 486.0972\n",
      "Epoch 7/200\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 378.2157\n",
      "Epoch 8/200\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 290.3551\n",
      "Epoch 9/200\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 219.9044\n",
      "Epoch 10/200\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 164.1908\n",
      "Epoch 11/200\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 120.5873\n",
      "Epoch 12/200\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 87.1968\n",
      "Epoch 13/200\n",
      "57/57 [==============================] - 0s 979us/step - loss: 62.0472\n",
      "Epoch 14/200\n",
      "57/57 [==============================] - 0s 919us/step - loss: 43.3983\n",
      "Epoch 15/200\n",
      "57/57 [==============================] - 0s 920us/step - loss: 29.7842\n",
      "Epoch 16/200\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 20.1686\n",
      "Epoch 17/200\n",
      "57/57 [==============================] - 0s 916us/step - loss: 13.5193\n",
      "Epoch 18/200\n",
      "57/57 [==============================] - 0s 898us/step - loss: 8.8658\n",
      "Epoch 19/200\n",
      "57/57 [==============================] - 0s 945us/step - loss: 5.7794\n",
      "Epoch 20/200\n",
      "57/57 [==============================] - 0s 906us/step - loss: 3.7560\n",
      "Epoch 21/200\n",
      "57/57 [==============================] - 0s 907us/step - loss: 2.4450\n",
      "Epoch 22/200\n",
      "57/57 [==============================] - 0s 913us/step - loss: 1.6217\n",
      "Epoch 23/200\n",
      "57/57 [==============================] - 0s 944us/step - loss: 1.1270\n",
      "Epoch 24/200\n",
      "57/57 [==============================] - 0s 924us/step - loss: 0.8044\n",
      "Epoch 25/200\n",
      "57/57 [==============================] - 0s 903us/step - loss: 0.6005\n",
      "Epoch 26/200\n",
      "57/57 [==============================] - 0s 923us/step - loss: 0.5502\n",
      "Epoch 27/200\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.4825\n",
      "Epoch 28/200\n",
      "57/57 [==============================] - 0s 956us/step - loss: 0.4472\n",
      "Epoch 29/200\n",
      "57/57 [==============================] - 0s 937us/step - loss: 0.4358\n",
      "Epoch 30/200\n",
      "57/57 [==============================] - 0s 928us/step - loss: 0.4360\n",
      "Epoch 31/200\n",
      "57/57 [==============================] - 0s 894us/step - loss: 0.4206\n",
      "Epoch 32/200\n",
      "57/57 [==============================] - 0s 927us/step - loss: 0.4226\n",
      "Epoch 33/200\n",
      "57/57 [==============================] - 0s 910us/step - loss: 0.4231\n",
      "Epoch 34/200\n",
      "57/57 [==============================] - 0s 923us/step - loss: 0.4044\n",
      "Epoch 35/200\n",
      "57/57 [==============================] - 0s 902us/step - loss: 0.4164\n",
      "Epoch 36/200\n",
      "57/57 [==============================] - 0s 960us/step - loss: 0.4014\n",
      "Epoch 37/200\n",
      "57/57 [==============================] - 0s 947us/step - loss: 0.4476\n",
      "Epoch 38/200\n",
      "57/57 [==============================] - 0s 902us/step - loss: 0.4023\n",
      "Epoch 39/200\n",
      "57/57 [==============================] - 0s 977us/step - loss: 0.4174\n",
      "Epoch 40/200\n",
      "57/57 [==============================] - 0s 928us/step - loss: 0.4115\n",
      "Epoch 41/200\n",
      "57/57 [==============================] - 0s 940us/step - loss: 0.4081\n",
      "Epoch 42/200\n",
      "57/57 [==============================] - 0s 927us/step - loss: 0.4066\n",
      "Epoch 43/200\n",
      "57/57 [==============================] - 0s 961us/step - loss: 0.4365\n",
      "Epoch 44/200\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.4146\n",
      "Epoch 45/200\n",
      "57/57 [==============================] - 0s 962us/step - loss: 0.4130\n",
      "Epoch 46/200\n",
      "57/57 [==============================] - 0s 991us/step - loss: 0.4200\n",
      "Epoch 47/200\n",
      "57/57 [==============================] - 0s 988us/step - loss: 0.4146\n",
      "Epoch 48/200\n",
      "57/57 [==============================] - 0s 971us/step - loss: 0.4232\n",
      "Epoch 49/200\n",
      "57/57 [==============================] - 0s 893us/step - loss: 0.4184\n",
      "Epoch 50/200\n",
      "57/57 [==============================] - 0s 929us/step - loss: 0.4120\n",
      "Epoch 51/200\n",
      "57/57 [==============================] - 0s 894us/step - loss: 0.4157\n",
      "Epoch 52/200\n",
      "57/57 [==============================] - 0s 941us/step - loss: 0.4249\n",
      "Epoch 53/200\n",
      "57/57 [==============================] - 0s 889us/step - loss: 0.4157\n",
      "Epoch 54/200\n",
      "57/57 [==============================] - 0s 936us/step - loss: 0.4436\n",
      "Epoch 55/200\n",
      "57/57 [==============================] - 0s 938us/step - loss: 0.3914\n",
      "Epoch 56/200\n",
      "57/57 [==============================] - 0s 908us/step - loss: 0.4207\n",
      "Epoch 57/200\n",
      "57/57 [==============================] - 0s 927us/step - loss: 0.3963\n",
      "Epoch 58/200\n",
      "57/57 [==============================] - 0s 892us/step - loss: 0.4206\n",
      "Epoch 59/200\n",
      "57/57 [==============================] - 0s 947us/step - loss: 0.4188\n",
      "Epoch 60/200\n",
      "57/57 [==============================] - 0s 939us/step - loss: 0.4111\n",
      "Epoch 61/200\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.3966\n",
      "Epoch 62/200\n",
      "57/57 [==============================] - 0s 939us/step - loss: 0.4158\n",
      "Epoch 63/200\n",
      "57/57 [==============================] - 0s 918us/step - loss: 0.4163\n",
      "Epoch 64/200\n",
      "57/57 [==============================] - 0s 962us/step - loss: 0.4182\n",
      "Epoch 65/200\n",
      "57/57 [==============================] - 0s 927us/step - loss: 0.4424\n",
      "Epoch 66/200\n",
      "57/57 [==============================] - 0s 946us/step - loss: 0.4251\n",
      "OilP Noise Training...\n",
      "Epoch 1/200\n",
      "57/57 [==============================] - 0s 937us/step - loss: 49.7244\n",
      "Epoch 2/200\n",
      "57/57 [==============================] - 0s 916us/step - loss: 15.9075\n",
      "Epoch 3/200\n",
      "57/57 [==============================] - 0s 956us/step - loss: 4.1400\n",
      "Epoch 4/200\n",
      "57/57 [==============================] - 0s 923us/step - loss: 1.4596\n",
      "Epoch 5/200\n",
      "57/57 [==============================] - 0s 918us/step - loss: 1.0426\n",
      "Epoch 6/200\n",
      "57/57 [==============================] - 0s 944us/step - loss: 1.0245\n",
      "Epoch 7/200\n",
      "57/57 [==============================] - 0s 910us/step - loss: 0.9708\n",
      "Epoch 8/200\n",
      "57/57 [==============================] - 0s 936us/step - loss: 1.0082\n",
      "Epoch 9/200\n",
      "57/57 [==============================] - 0s 894us/step - loss: 1.0366\n",
      "Epoch 10/200\n",
      "57/57 [==============================] - 0s 915us/step - loss: 1.0395\n",
      "Epoch 11/200\n",
      "57/57 [==============================] - 0s 928us/step - loss: 1.0079\n",
      "Epoch 12/200\n",
      "57/57 [==============================] - 0s 963us/step - loss: 0.9971\n",
      "Epoch 13/200\n",
      "57/57 [==============================] - 0s 922us/step - loss: 1.0101\n",
      "Epoch 14/200\n",
      "57/57 [==============================] - 0s 922us/step - loss: 1.0306\n",
      "Epoch 15/200\n",
      "57/57 [==============================] - 0s 963us/step - loss: 0.9901\n",
      "Epoch 16/200\n",
      "57/57 [==============================] - 0s 895us/step - loss: 0.9873\n",
      "Epoch 17/200\n",
      "57/57 [==============================] - 0s 924us/step - loss: 1.0388\n",
      "Epoch 18/200\n",
      "57/57 [==============================] - 0s 903us/step - loss: 1.0572\n",
      "Epoch 19/200\n",
      "57/57 [==============================] - 0s 893us/step - loss: 1.0222\n",
      "Epoch 20/200\n",
      "57/57 [==============================] - 0s 936us/step - loss: 1.0489\n",
      "Epoch 21/200\n",
      "57/57 [==============================] - 0s 902us/step - loss: 0.9745\n",
      "Epoch 22/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 932us/step - loss: 1.0235\n",
      "Epoch 23/200\n",
      "57/57 [==============================] - 0s 958us/step - loss: 1.0406\n",
      "Epoch 24/200\n",
      "57/57 [==============================] - 0s 933us/step - loss: 0.9874\n",
      "Epoch 25/200\n",
      "57/57 [==============================] - 0s 923us/step - loss: 1.0178\n",
      "Epoch 26/200\n",
      "57/57 [==============================] - 0s 899us/step - loss: 0.9743\n",
      "Epoch 27/200\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 1.0114\n",
      "Epoch 28/200\n",
      "57/57 [==============================] - 0s 918us/step - loss: 0.9956\n",
      "Epoch 29/200\n",
      "57/57 [==============================] - 0s 901us/step - loss: 1.0273\n",
      "Epoch 30/200\n",
      "57/57 [==============================] - 0s 944us/step - loss: 1.0000\n",
      "Epoch 31/200\n",
      "57/57 [==============================] - 0s 935us/step - loss: 0.9866\n",
      "Epoch 32/200\n",
      "57/57 [==============================] - 0s 877us/step - loss: 1.0125\n",
      "Epoch 33/200\n",
      "57/57 [==============================] - 0s 926us/step - loss: 0.9890\n",
      "Epoch 34/200\n",
      "57/57 [==============================] - 0s 908us/step - loss: 1.0185\n",
      "Epoch 35/200\n",
      "57/57 [==============================] - 0s 897us/step - loss: 1.0206\n",
      "Epoch 36/200\n",
      "57/57 [==============================] - 0s 907us/step - loss: 1.0383\n",
      "Epoch 37/200\n",
      "57/57 [==============================] - 0s 901us/step - loss: 0.9994\n",
      "Epoch 38/200\n",
      "57/57 [==============================] - 0s 901us/step - loss: 1.0226\n",
      "Epoch 39/200\n",
      "57/57 [==============================] - 0s 901us/step - loss: 1.0588\n",
      "Epoch 40/200\n",
      "57/57 [==============================] - 0s 918us/step - loss: 0.9677\n",
      "Epoch 41/200\n",
      "57/57 [==============================] - 0s 914us/step - loss: 1.0213\n",
      "Epoch 42/200\n",
      "57/57 [==============================] - 0s 889us/step - loss: 1.0127\n",
      "Epoch 43/200\n",
      "57/57 [==============================] - 0s 906us/step - loss: 1.0270\n",
      "Epoch 44/200\n",
      "57/57 [==============================] - 0s 911us/step - loss: 1.0081\n",
      "Epoch 45/200\n",
      "57/57 [==============================] - 0s 928us/step - loss: 1.0087\n",
      "Epoch 46/200\n",
      "57/57 [==============================] - 0s 913us/step - loss: 1.0186\n",
      "Epoch 47/200\n",
      "57/57 [==============================] - 0s 940us/step - loss: 1.0586\n",
      "Epoch 48/200\n",
      "57/57 [==============================] - 0s 902us/step - loss: 1.0629\n",
      "Epoch 49/200\n",
      "57/57 [==============================] - 0s 950us/step - loss: 0.9879\n",
      "Epoch 50/200\n",
      "57/57 [==============================] - 0s 909us/step - loss: 1.0193\n",
      "Epoch 51/200\n",
      "57/57 [==============================] - 0s 897us/step - loss: 1.0284\n",
      "Epoch 52/200\n",
      "57/57 [==============================] - 0s 952us/step - loss: 1.0221\n",
      "Epoch 53/200\n",
      "57/57 [==============================] - 0s 954us/step - loss: 1.0453\n"
     ]
    }
   ],
   "source": [
    "generate_models = True\n",
    "\n",
    "# Inputs must be separate because we're going to concatenate it\n",
    "rpm_input = tf.keras.Input(shape=(1))\n",
    "rpm_diff_input = tf.keras.Input(shape=(1))\n",
    "rpm_static_input = tf.keras.Input(shape=(1))\n",
    "fflow_input = tf.keras.Input(shape=(1))\n",
    "egt1_input = tf.keras.Input(shape=(1))\n",
    "egt1_diff_input = tf.keras.Input(shape=(1))\n",
    "egt1_static_input = tf.keras.Input(shape=(1))\n",
    "vspd_input = tf.keras.Input(shape=(1))\n",
    "constant_input = tf.keras.Input(shape=(1))\n",
    "ias_input = tf.keras.Input(shape=(1))\n",
    "alt_input = tf.keras.Input(shape=(1))\n",
    "oilp_input = tf.keras.Input(shape=(1))\n",
    "\n",
    "error = 100\n",
    "\n",
    "# This is the encoder part of an autoencoder to guess what throttle should be based on all inputs into fflow (and fflow itself)\n",
    "throttle_encoder_concat = layers.Concatenate(axis=1)([ias_input, alt_input, fflow_input, rpm_input])\n",
    "throttle_encoder_z = layers.Dense(200)(throttle_encoder_concat)\n",
    "throttle_encoder_z = layers.Dense(200)(throttle_encoder_z)\n",
    "throttle_encoder_z = layers.Dense(1)(throttle_encoder_z)\n",
    "throttle_encoder = tf.keras.Model([ias_input, alt_input, fflow_input, rpm_input], throttle_encoder_z, name=\"throttle_enc\")\n",
    "throttle_encoder.summary()\n",
    "\n",
    "# The decoder part of the throttle autoencoder is the function to calculate fflow\n",
    "throttle_decoder_concat = layers.Concatenate(axis=1)([throttle_encoder_z, ias_input, alt_input, rpm_input])\n",
    "throttle_decoder_z = layers.Dense(200)(throttle_decoder_concat)\n",
    "throttle_decoder_z = layers.Dense(200)(throttle_decoder_z)\n",
    "throttle_decoder_z = layers.Dense(1)(throttle_decoder_z)\n",
    "throttle_decoder = tf.keras.Model([throttle_encoder_z, ias_input, alt_input, rpm_input], throttle_decoder_z, name=\"throttle_dec\")\n",
    "throttle_decoder.summary()\n",
    "\n",
    "# This is the function to calculate the expected value of EGTStat. We will add noise to this later.\n",
    "egt_stat_concat = layers.Concatenate(axis=1)([fflow_input, rpm_input])\n",
    "egt_stat_z = layers.Dense(200)(egt_stat_concat)\n",
    "egt_stat_z = layers.Dense(200)(egt_stat_z)\n",
    "egt_stat_z = layers.Dense(1)(egt_stat_z)\n",
    "egt_stat = tf.keras.Model([fflow_input, rpm_input], egt_stat_z, name=\"egt_stat\")\n",
    "egt_stat.summary()\n",
    "\n",
    "# This is the timescale for the change in EGT (dEGTdt) it should be a constant and positive (hence the sigmoid activation)\n",
    "egt_ts_concat = layers.Concatenate(axis=1)([constant_input])\n",
    "egt_ts_z = layers.Dense(1, activation='sigmoid')(egt_ts_concat)\n",
    "egt_ts = tf.keras.Model([constant_input], egt_ts_z, name=\"egt_ts\")\n",
    "egt_ts.summary()\n",
    "\n",
    "# This is the variance of EGT from the predicted value of EGTStat\n",
    "egt_stat_var_concat = layers.Concatenate(axis=1)([constant_input])\n",
    "egt_stat_var_z = layers.Dense(1)(egt_stat_var_concat)\n",
    "egt_stat_var = tf.keras.Model([constant_input], egt_stat_var_z, name=\"egt_stat_var\")\n",
    "egt_stat_var.summary()\n",
    "\n",
    "# This is the function to estimate OilP (oil pressure) which is dependent on the RPM\n",
    "oilp_z = layers.Dense(20)(rpm_input)\n",
    "oilp_z = layers.Dense(1)(oilp_z)\n",
    "oilp = tf.keras.Model([rpm_input], oilp_z, name=\"oilp\")\n",
    "oilp.summary()\n",
    "\n",
    "# This is the variance of OilP from the predicted expected value of OilP\n",
    "oilp_var_concat = layers.Concatenate(axis=1)([constant_input])\n",
    "oilp_var_z = layers.Dense(1)(oilp_var_concat)\n",
    "oilp_var = tf.keras.Model([constant_input], oilp_var_z, name=\"oilp_var\")\n",
    "oilp_var.summary()\n",
    "\n",
    "\n",
    "rpm_input_data = np.reshape(training_data_after[:,0], [training_data_after[:,0].shape[0],1])\n",
    "egt_input_data = np.reshape(training_data_after[:,1], [training_data_after[:,1].shape[0],1])\n",
    "fflow_input_data = np.reshape(training_data_after[:,2], [training_data_after[:,2].shape[0],1])\n",
    "vspd_input_data = np.reshape(training_data_after[:,3], [training_data_after[:,3].shape[0],1])\n",
    "alt_input_data = np.reshape(training_data_after[:,4], [training_data_after[:,4].shape[0],1])\n",
    "ias_input_data = np.reshape(training_data_after[:,5], [training_data_after[:,5].shape[0],1])\n",
    "rpm_diff_input_data = np.reshape(training_data_after[:,6], [training_data_after[:,6].shape[0],1])\n",
    "egt1_diff_input_data = np.reshape(training_data_after[:,7], [training_data_after[:,7].shape[0],1])\n",
    "oilp_input_data = np.reshape(training_data_after[:,8], [training_data_after[:,8].shape[0],1])\n",
    "constant_ones = np.ones(training_data_after[:,0].shape)\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                            patience=20,\n",
    "                                            restore_best_weights=True)\n",
    "\n",
    "# Learn the latent variable Throttle and the function to calculate fflow\n",
    "throttle_model = ThrottleModel([fflow_input, ias_input, alt_input, rpm_input], throttle_encoder, throttle_decoder)\n",
    "if generate_models:\n",
    "    print(\"Throttle/FFlow Training...\")\n",
    "    throttle_model.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "    throttle_model.fit([fflow_input_data,\n",
    "                     ias_input_data,\n",
    "                     alt_input_data,\n",
    "                     rpm_input_data],\n",
    "             epochs=200, batch_size=1000, verbose=1, callbacks=[callback])\n",
    "    throttle_model.save_weights('throttle_model.weights')\n",
    "else:\n",
    "    throttle_model.load_weights('throttle_model.weights')\n",
    "\n",
    "# Learn the OilP values\n",
    "\n",
    "if generate_models:\n",
    "    print(\"OilP Training...\")\n",
    "    oilp.compile(loss='mae', optimizer=tf.keras.optimizers.Adam())\n",
    "    oilp.fit([rpm_input_data], oilp_input_data, epochs=200, batch_size=1000, verbose=1, callbacks=[callback], validation_split=0.2)\n",
    "    oilp.save('oilp.model')\n",
    "else:\n",
    "    oilp = tf.keras.models.load_model('oilp.model')\n",
    "\n",
    "oilp_predicted = oilp.predict([rpm_input_data], verbose=0)\n",
    "\n",
    "# Learn the egt stationary values\n",
    "\n",
    "if generate_models:\n",
    "    print(\"EGT Stationary Training...\")\n",
    "    egt_stat.compile(loss='mae', optimizer=tf.keras.optimizers.Adam())\n",
    "    egt_stat.fit([fflow_input_data,rpm_input_data], egt_input_data, epochs=200, batch_size=1000, verbose=1, callbacks=[callback], validation_split=0.2)\n",
    "    egt_stat.save('egt_stat.model')\n",
    "else:\n",
    "    egt_stat = tf.keras.models.load_model('egt_stat.model')\n",
    "\n",
    "egt_stat_predicted = egt_stat.predict([fflow_input_data, rpm_input_data], verbose=0)\n",
    "\n",
    "# Learn the timescale of dEGTdt\n",
    "egt_deriv_model = DerivModel([egt1_static_input, egt1_input, egt1_diff_input,constant_input], egt_ts)\n",
    "if generate_models:\n",
    "    print(\"Timescale Training...\")\n",
    "    egt_deriv_model.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "    egt_deriv_model.fit([egt_stat_predicted,\n",
    "                     egt_input_data,\n",
    "                     egt1_diff_input_data,\n",
    "                     constant_ones],\n",
    "             epochs=200, batch_size=1000, verbose=1, callbacks=[callback])\n",
    "    egt_deriv_model.save_weights('egt_ts_model.weights')\n",
    "else:\n",
    "    egt_deriv_model.load_weights('egt_ts_model.weights')\n",
    "\n",
    "# Learn the (assumed) constant variance of EGTStat\n",
    "egt_noise_model = NoiseModel([constant_input, egt_stat_z, egt1_input], egt_stat_var)\n",
    "if generate_models:\n",
    "    print(\"EGT Noise Training...\")\n",
    "    egt_noise_model.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "    egt_noise_model.fit([constant_ones,\n",
    "                     egt_stat_predicted,\n",
    "                     egt_input_data],\n",
    "             epochs=200, batch_size=1000, verbose=1, callbacks=[callback])\n",
    "    egt_noise_model.save_weights('egt_noise_model.weights')\n",
    "else:\n",
    "    egt_noise_model.load_weights('egt_noise_model.weights')\n",
    "\n",
    "# Learn the (assumed) constant variance of OilP\n",
    "oilp_noise_model = NoiseModel([constant_input, oilp_z, oilp_input], oilp_var)\n",
    "if generate_models:\n",
    "    print(\"OilP Noise Training...\")\n",
    "    oilp_noise_model.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "    oilp_noise_model.fit([constant_ones,\n",
    "                     oilp_predicted,\n",
    "                     oilp_input_data],\n",
    "             epochs=200, batch_size=1000, verbose=1, callbacks=[callback])\n",
    "    oilp_noise_model.save_weights('oilp_noise_model.weights')\n",
    "else:\n",
    "    oilp_noise_model.load_weights('oilp_noise_model.weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHzWfEPayo9A"
   },
   "source": [
    "Now we want to build the Causal Jazz simulation. First define the functions of the SCM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2TpCdXlKyw7X"
   },
   "outputs": [],
   "source": [
    "def prepareDataForPrediction(y):\n",
    "    y = np.array(y)\n",
    "    prepared = []\n",
    "    for d in range(y.shape[1]):\n",
    "        data = y[:,d]\n",
    "        data = np.reshape(data, [data.shape[0],1])\n",
    "        prepared = prepared + [data]\n",
    "\n",
    "    return prepared\n",
    "\n",
    "# Estimate the constant timescales to be applied to dRPMdt and dEGTdt in rpm_func and egt_func below\n",
    "# dEGTdt timescale\n",
    "egt_ts_predicted = egt_deriv_model.ts_model.predict([constant_ones], verbose=0)\n",
    "egt_ts_constant = egt_ts_predicted[0,0]\n",
    "\n",
    "# This is our estimated SCM\n",
    "def fflow_func(y):\n",
    "    data = prepareDataForPrediction(y)\n",
    "    fflow = throttle_model.throttle_decoder.predict(data,verbose=0)\n",
    "    return np.transpose(np.array([fflow[:,0]]))\n",
    "\n",
    "def egt_stationary_func(y):\n",
    "    #data[0] is the noise and is added separately\n",
    "    data = prepareDataForPrediction(np.array(y)[:,1:])\n",
    "    noise_data = prepareDataForPrediction(y)\n",
    "    egt_stat_out = egt_stat.predict(data, verbose=0)\n",
    "    return np.transpose(np.array([np.array(egt_stat_out[:,0]) + np.array(noise_data)[0,:,0]]))\n",
    "\n",
    "def egt_func(y):\n",
    "    data = prepareDataForPrediction(y)\n",
    "    egt_prime = egt_ts_constant * (np.array(data)[1,:,0] - np.array(data)[0,:,0])\n",
    "    egt_new = np.array(data)[0,:,0] + egt_prime\n",
    "    return np.transpose(np.array([egt_new]))\n",
    "\n",
    "def oilp_func(y):\n",
    "    #data[0] is the noise and is added separately\n",
    "    data = prepareDataForPrediction(np.array(y)[:,1:])\n",
    "    noise_data = prepareDataForPrediction(y)\n",
    "    oilp_out = oilp.predict(data, verbose=0)\n",
    "    return np.transpose(np.array([np.array(oilp_out[:,0]) + np.array(noise_data)[0,:,0]]))\n",
    "\n",
    "trans_fflow_func = transition(fflow_func, 4)\n",
    "trans_egtstat_func = transition(egt_stationary_func, 3)\n",
    "trans_egt_func = transition(egt_func, 2)\n",
    "trans_oilp_func = transition(oilp_func, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9YLN_2WzptI"
   },
   "source": [
    "Now build the initial distributions. res is the grid resolution of all discretised distributions. As most of the data are normalised, this amounts to a cell width of approximately 0.02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FtxFC6Edz3UX"
   },
   "outputs": [],
   "source": [
    "res = 50\n",
    "\n",
    "# From the full data, predict the distribution of throttle and calculate the constant variance for EGTNoise, RPMNoise\n",
    "# egt_stat_predicted and rpm_stat_predicted have already been generated as they were used to learn RPMNoise and EGTNoise\n",
    "# dEGTdt timescale and dRPMdt timescale have already been calculated to be used in egt_func and rpm_func\n",
    "\n",
    "# Throttle\n",
    "throttle_predicted = throttle_model.throttle_encoder.predict([ias_input_data, alt_input_data, fflow_input_data, rpm_input_data],verbose=0)\n",
    "\n",
    "# EGTNoise\n",
    "egt_stat_var_predicted = egt_noise_model.var_model.predict([constant_ones],verbose=0) # Hopefully, noise_encoder has encoded a nice gaussianish variable (not guaranteed at all)\n",
    "egt_stat_var_constant = egt_stat_var_predicted[0,0]\n",
    "\n",
    "# OilPNoise\n",
    "oilp_var_predicted = oilp_noise_model.var_model.predict([constant_ones],verbose=0) # Hopefully, noise_encoder has encoded a nice gaussianish variable (not guaranteed at all)\n",
    "oilp_var_constant = oilp_var_predicted[0,0]\n",
    "\n",
    "# Number of points to deal with at a time\n",
    "num_points = 100\n",
    "\n",
    "# First, sample num_points values from the throttle distribution\n",
    "throttle_pmf = build_pmf(throttle_predicted[:,0])\n",
    "throttle_cmf = build_cmf(throttle_pmf)\n",
    "\n",
    "\n",
    "# Get the starting data for our observed variables\n",
    "data_items = normaliseAndFormatData(getFlightDataAtTimestep(flight_data_df_after, 1))\n",
    "red_ias_input_data = data_items[5][:,0]\n",
    "red_alt_input_data = data_items[4][:,0]\n",
    "red_rpm_input_data = data_items[0][:,0]\n",
    "red_egt_input_data = data_items[1][:,0]\n",
    "red_oilp_input_data = data_items[8][:,0]\n",
    "red_egt_stat_predicted = egt_stat_predicted[:,0]\n",
    "\n",
    "throttle_noise_basic_sampled = sample_from_cmf(throttle_cmf, red_ias_input_data.shape[0], np.min(throttle_predicted), np.max(throttle_predicted))\n",
    "\n",
    "# The resolution of our grids - right now, we only allow a single value for all dimensions (there's a bug when it's different)\n",
    "\n",
    "\n",
    "# Build the initial distributions based on the initial data\n",
    "throttle_noise_pmf = pmf(build_pmf(throttle_noise_basic_sampled,num_intervals=res), np.array([(np.max(throttle_noise_basic_sampled)-np.min(throttle_noise_basic_sampled))/res]), 0.00001)\n",
    "ias_pmf = pmf(build_pmf(red_ias_input_data,num_intervals=res), np.array([(np.max(red_ias_input_data)-np.min(red_ias_input_data))/res]), 0.00001)\n",
    "alt_pmf = pmf(build_pmf(red_alt_input_data,num_intervals=res),  np.array([(np.max(red_alt_input_data)-np.min(red_alt_input_data))/res]), 0.00001)\n",
    "rpm_input_pmf = pmf(build_pmf(red_rpm_input_data,num_intervals=res),np.array([(np.max(red_rpm_input_data)-np.min(red_rpm_input_data))/res]), 0.000001)\n",
    "egt_input_pmf = pmf(build_pmf(red_egt_input_data,num_intervals=res), np.array([(np.max(red_egt_input_data)-np.min(red_egt_input_data))/res]), 0.000001)\n",
    "oilp_input_pmf = pmf(build_pmf(red_oilp_input_data,num_intervals=res),  np.array([(np.max(red_oilp_input_data)-np.min(red_oilp_input_data))/res]), 0.00001)\n",
    "\n",
    "# Build the initial noise dis mributions\n",
    "egt_stat_noise_points = np.reshape(np.random.normal(loc=0.0,scale=np.sqrt(egt_stat_var_constant), size=100000), (100000))\n",
    "egt_stat_noise_pmf = pmf(np.array([]), np.array([(np.max(egt_stat_noise_points)-np.min(egt_stat_noise_points))/res]), 0.000001)\n",
    "egt_stat_noise_points = np.reshape(egt_stat_noise_points, (100000,1))\n",
    "egt_stat_noise_pmf.generateInitialDistribtionFromSample(egt_stat_noise_points)\n",
    "\n",
    "oilp_noise_points = np.reshape(np.random.normal(loc=0.0,scale=np.sqrt(oilp_var_constant), size=100000), (100000))\n",
    "oilp_noise_pmf = pmf(np.array([]), np.array([(np.max(oilp_noise_points)-np.min(oilp_noise_points))/res]), 0.00001)\n",
    "oilp_noise_points = np.reshape(oilp_noise_points, (100000,1))\n",
    "oilp_noise_pmf.generateInitialDistribtionFromSample(oilp_noise_points)\n",
    "\n",
    "# Build template distributions for the remaining variables\n",
    "fflow_pmf_template = pmf(np.array([1.0]), np.array([(np.max(fflow_input_data)-np.min(fflow_input_data))/res]), 0.000001)\n",
    "egt_stat_template = pmf(np.array([1.0]), np.array([(np.max(egt_stat_predicted[:,0])-np.min(egt_stat_predicted[:,0]))/res]), 0.000001)\n",
    "oilp_template = pmf(np.array([1.0]), np.array([(np.max(oilp_input_data[:,0])-np.min(oilp_input_data[:,0]))/res]), 0.000001)\n",
    "rpm_pmf_template = pmf(np.array([1.0]), np.array([(np.max(rpm_input_data)-np.min(rpm_input_data))/res]), 0.000001)\n",
    "\n",
    "# Starting distributions for egt and rpm\n",
    "egt_noise_points = np.reshape(np.random.normal(loc=-0.2,scale=0.05, size=100000), (100000))\n",
    "egt_noise_pmf = pmf(np.array([]), np.array([1.0/res]), 0.000001)\n",
    "egt_noise_points = np.reshape(egt_noise_points, (100000,1))\n",
    "egt_noise_pmf.generateInitialDistribtionFromSample(egt_noise_points)\n",
    "\n",
    "rpm_noise_points = np.reshape(np.random.normal(loc=0.2,scale=0.05, size=100000), (100000))\n",
    "rpm_noise_pmf = pmf(np.array([]), np.array([(np.max(rpm_noise_points)-np.min(rpm_noise_points))/res]), 0.000001)\n",
    "rpm_noise_points = np.reshape(rpm_noise_points, (100000,1))\n",
    "rpm_noise_pmf.generateInitialDistribtionFromSample(rpm_noise_points)\n",
    "\n",
    "# Build initial distribution of observed variables and throttle\n",
    "data_points = np.stack([throttle_noise_basic_sampled, red_alt_input_data, red_ias_input_data, red_rpm_input_data, red_egt_input_data, red_oilp_input_data])\n",
    "_, _, _, cell_widths = calculateCellGridMetrics(data_points, [res,res,res,res,res,res])\n",
    "start_pmf = pmf(np.array([]), cell_widths, _mass_epsilon=0.000001)\n",
    "in_data_points = np.transpose(data_points)\n",
    "start_pmf.generateInitialDistribtionFromSample(in_data_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5vwYztx1D_3"
   },
   "source": [
    "Flags to say what to simulation and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "i-JM3i3g1HFv"
   },
   "outputs": [],
   "source": [
    "plot_data_fixed = False\n",
    "plot_data_issue = True\n",
    "plot_cj = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-Smbmf00N-R"
   },
   "source": [
    "Define the TEDAG and its TEDAG_FUNCTIONS.\n",
    "Define any interventions. This should include interventions to set the initial distributions at time 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VB40qraT1IXP"
   },
   "source": [
    "Simulate! Causal sample points are plotted in blue, post-maintenance (fixed) data points in green, and pre-maintenance (issue) data points in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "q8C1FrbI1NRf",
    "outputId": "dde08728-a6e4-48bd-9017-1689e83b0daf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 183] Cannot create a file when that file already exists: 'out_data'\n",
      "Continuing...\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "tedag_func_throttle = TEDAG_FUNCTION(['Throttle'], 'Throttle', 1, None, throttle_noise_pmf)\n",
    "tedag_func_egt_noise = TEDAG_FUNCTION(['EGTNoise'], 'EGTNoise', 1, None, egt_stat_noise_pmf)\n",
    "tedag_func_oilp_noise = TEDAG_FUNCTION(['OilPNoise'], 'OilPNoise', 1, None, oilp_noise_pmf)\n",
    "\n",
    "# We're just ignoring changes in IAS, AltMSL, and RPM for now so treat them like constants\n",
    "tedag_func_ias = TEDAG_FUNCTION(['IAS'], 'IAS', 1, None, ias_pmf)\n",
    "tedag_func_altmsl = TEDAG_FUNCTION(['AltMSL'], 'AltMSL', 1, None, alt_pmf)\n",
    "tedag_func_rpm = TEDAG_FUNCTION(['E1 RPM'], 'E1 RPM', 1, None, rpm_pmf_template)\n",
    "\n",
    "# Functions which are calculated \"instantly\" : less than a single dt.\n",
    "tedag_func_fflow = TEDAG_FUNCTION(['Throttle', 'IAS', 'AltMSL', 'E1 RPM'], 'E1 FFlow', 0, trans_fflow_func, fflow_pmf_template)\n",
    "tedag_func_egt_stat = TEDAG_FUNCTION(['EGTNoise', 'E1 FFlow', 'E1 RPM'], 'EGTStat', 0, trans_egtstat_func, egt_stat_template)\n",
    "tedag_func_oilp = TEDAG_FUNCTION(['OilPNoise','E1 RPM'], 'E1 OilP', 0, trans_oilp_func, oilp_template)\n",
    "\n",
    "# Currently we don't store dEGTdt and just calculate it during tedag_func_egt\n",
    "# but if we wished, we could make it explicit which would be nice to see and also to intervene on\n",
    "#tedag_func_egt_dt = TEDAG_FUNCTION(['EGTStat', 'E1 EGT1'], 'dEGTdt', 0)\n",
    "\n",
    "# Functions which are calculated over multiples of dts\n",
    "tedag_func_egt = TEDAG_FUNCTION(['E1 EGT1', 'EGTStat'], 'E1 EGT1', 1, trans_egt_func, egt_input_pmf)\n",
    "\n",
    "\n",
    "tedag = TEDAG(1.0, [tedag_func_throttle, tedag_func_egt_noise, tedag_func_ias, tedag_func_rpm,\n",
    "                    tedag_func_altmsl, tedag_func_fflow, tedag_func_oilp, tedag_func_oilp_noise,\n",
    "                    tedag_func_egt_stat, tedag_func_egt], ['E1 RPM', 'E1 EGT1'])\n",
    "\n",
    "tedag.addIntervention(['Throttle','AltMSL','IAS', 'E1 RPM', 'E1 EGT1', 'E1 OilP'], 0, start_pmf)\n",
    "tedag.addIntervention(['EGTNoise'], 0, egt_stat_noise_pmf)\n",
    "tedag.addIntervention(['OilPNoise'], 0, oilp_noise_pmf)\n",
    "tedag.addIntervention(['E1 RPM'], 50, rpm_noise_pmf)\n",
    "\n",
    "if plot_dist:\n",
    "    mc_vis = Visualiser(2)\n",
    "    mc_vis.setupVisuliser()\n",
    "\n",
    "if plot_data_issue:\n",
    "  current_timestep_df_before = flight_data_df_before[['E1 RPM', 'E1 EGT1','timestep']]\n",
    "if plot_data_fixed:\n",
    "  current_timestep_df_after = flight_data_df_after[['E1 RPM', 'E1 EGT1','timestep']]\n",
    "\n",
    "vis_key_inds = [0, 1]\n",
    "vis_val_max = [0.0, 0.0]\n",
    "vis_val_min = [0.0, 0.0]\n",
    "\n",
    "iteration = 0\n",
    "# Loop through the timesteps up to the maximum we require. Note all flights begin in the \"grounded\" mode\n",
    "# so due to our IAS filter above, we will intially see no points until flights begin taking off\n",
    "for current_timestep in range(max_num_timesteps):\n",
    "    \n",
    "    if plot_dist:\n",
    "        mc_vis.beginRendering()\n",
    "\n",
    "    #fig = plt.figure()\n",
    "    #plt.xlim([-0.5,0.5])\n",
    "    #plt.ylim([-0.5,0.5])\n",
    "    #plt.xlabel(\"E1 RPM\")\n",
    "    #plt.ylabel(\"E1 EGT1\")\n",
    "\n",
    "    if plot_cj:\n",
    "      # tedag step\n",
    "      while tedag.findNextFunctionAndApply(current_timestep):\n",
    "          continue\n",
    "\n",
    "      tedag_pmf = tedag.getPmfForIteration(['E1 RPM', 'E1 EGT1'], current_timestep)\n",
    "      if tedag_pmf is not None:\n",
    "          sampled_points = np.array(tedag_pmf.pmf.sample(num_points))\n",
    "          node_indices = [[n.key for n in tedag_pmf.nodes].index(a+str(current_timestep)) for a in ['E1 RPM', 'E1 EGT1']]\n",
    "          if plot_dist:\n",
    "              tedag_pmf.pmf.drawContinued(grid_min_override=[-0.55,-0.55], grid_max_override=[0.55,0.55], grid_res_override=[res,res], vis=mc_vis, vis_dimensions=node_indices)\n",
    "          #plt.plot(np.array(sampled_points)[:,node_indices[0]], np.array(sampled_points)[:,node_indices[1]], 'bo')\n",
    "\n",
    "    # Loop through all flights up the max we want to view\n",
    "    flight_count = 0\n",
    "\n",
    "    if plot_data_fixed:\n",
    "        current_timestep_df_a = current_timestep_df_after[current_timestep_df_after['timestep'] == current_timestep]\n",
    "        points_x = []\n",
    "        points_y = []\n",
    "        for flight_index in [a for a in current_timestep_df_a.index]:\n",
    "            if flight_count > num_points:\n",
    "                continue\n",
    "            flight_count += 1\n",
    "\n",
    "            if flight_index not in [a for a in current_timestep_df_a.index]:\n",
    "                continue\n",
    "\n",
    "            flight_timestep_vals = current_timestep_df_a.loc[flight_index].to_numpy().tolist()\n",
    "\n",
    "            # For the two variables we want, if either are NaN, just ignore this data point.\n",
    "            if (math.isnan(flight_timestep_vals[vis_key_inds[0]]) or math.isnan(flight_timestep_vals[vis_key_inds[1]])):\n",
    "                continue\n",
    "\n",
    "            # Calculate the screen-space location of the data point\n",
    "            val_pos = [0.0,0.0]\n",
    "\n",
    "            #print(flight_timestep_vals[vis_key_inds[0]], flight_timestep_vals[vis_key_inds[1]])\n",
    "            val_pos[0] = -0.5 + ((flight_timestep_vals[vis_key_inds[0]] - min_vals[0]) / max_vals[0])\n",
    "            val_pos[1] = -0.5 + ((flight_timestep_vals[vis_key_inds[1]] - min_vals[1]) / max_vals[1])\n",
    "\n",
    "            points_x += [val_pos[0]]\n",
    "            points_y += [val_pos[1]]\n",
    "            \n",
    "            if plot_dist:\n",
    "                val_pos[0] = (-1.0/1.1) + ((2.0/1.1)*(flight_timestep_vals[vis_key_inds[0]] - min_vals[0]) / max_vals[0])\n",
    "                val_pos[1] = (-1.0/1.1) + ((2.0/1.1)*(flight_timestep_vals[vis_key_inds[1]] - min_vals[1]) / max_vals[1])\n",
    "                mc_vis.square(pos=(val_pos[0], val_pos[1]), scale=(0.01,0.01), col=(0,0.8,0))\n",
    "\n",
    "        #plt.plot(points_x, points_y, 'gx')\n",
    "\n",
    "    flight_count = 0\n",
    "\n",
    "    if plot_data_issue:\n",
    "        current_timestep_df_b = current_timestep_df_before[current_timestep_df_before['timestep'] == current_timestep]\n",
    "        points_x = []\n",
    "        points_y = []\n",
    "        for flight_index in [a for a in current_timestep_df_b.index]:\n",
    "            if flight_count > num_points:\n",
    "                continue\n",
    "            flight_count += 1\n",
    "\n",
    "            if flight_index not in [a for a in current_timestep_df_b.index]:\n",
    "                continue\n",
    "\n",
    "            flight_timestep_vals = current_timestep_df_b.loc[flight_index].to_numpy().tolist()\n",
    "\n",
    "            # For the two variables we want, if either are NaN, just ignore this data point.\n",
    "            if (math.isnan(flight_timestep_vals[vis_key_inds[0]]) or math.isnan(flight_timestep_vals[vis_key_inds[1]])):\n",
    "                continue\n",
    "\n",
    "            # Calculate the screen-space location of the data point\n",
    "            val_pos = [0.0,0.0]\n",
    "\n",
    "            #print(flight_timestep_vals[vis_key_inds[0]], flight_timestep_vals[vis_key_inds[1]])\n",
    "            val_pos[0] = -0.5 + ((flight_timestep_vals[vis_key_inds[0]] - min_vals[0]) / max_vals[0])\n",
    "            val_pos[1] = -0.5 + ((flight_timestep_vals[vis_key_inds[1]] - min_vals[1]) / max_vals[1])\n",
    "\n",
    "            points_x += [val_pos[0]]\n",
    "            points_y += [val_pos[1]]\n",
    "            \n",
    "            if plot_dist:\n",
    "                val_pos[0] = (-1.0/1.1) + ((2.0/1.1)*(flight_timestep_vals[vis_key_inds[0]] - min_vals[0]) / max_vals[0])\n",
    "                val_pos[1] = (-1.0/1.1) + ((2.0/1.1)*(flight_timestep_vals[vis_key_inds[1]] - min_vals[1]) / max_vals[1])\n",
    "                mc_vis.square(pos=(val_pos[0], val_pos[1]), scale=(0.01,0.01), col=(0.8,0,0))\n",
    "\n",
    "        #plt.plot(np.array(points_x), np.array(points_y), 'r+')\n",
    "\n",
    "    #plt.show()\n",
    "    if plot_dist:\n",
    "        mc_vis.endRendering()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
