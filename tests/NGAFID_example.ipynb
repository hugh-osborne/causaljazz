{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install python dev tools and dask dataframes."
      ],
      "metadata": {
        "id": "Txilod5-gZ6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-dev-tools\n",
        "!python -m pip install \"dask[dataframe]\""
      ],
      "metadata": {
        "id": "d7yvTmMoga4t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4126d683-4689-4c91-db57-b3b50f5441a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting colab-dev-tools\n",
            "  Downloading colab_dev_tools-0.0.10-py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from colab-dev-tools) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from colab-dev-tools) (1.25.2)\n",
            "Collecting nvidia-ml-py3 (from colab-dev-tools)\n",
            "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->colab-dev-tools) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->colab-dev-tools) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->colab-dev-tools) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->colab-dev-tools) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->colab-dev-tools) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->colab-dev-tools) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->colab-dev-tools)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->colab-dev-tools)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->colab-dev-tools)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch->colab-dev-tools)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch->colab-dev-tools)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch->colab-dev-tools)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch->colab-dev-tools)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch->colab-dev-tools)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch->colab-dev-tools)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch->colab-dev-tools)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch->colab-dev-tools)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->colab-dev-tools) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->colab-dev-tools)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->colab-dev-tools) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->colab-dev-tools) (1.3.0)\n",
            "Building wheels for collected packages: nvidia-ml-py3\n",
            "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19172 sha256=2a3cada7b60fd6421a2d1e098daf65834242760195af99fc0bc437a71f325b89\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/d8/c0/46899f8be7a75a2ffd197a23c8797700ea858b9b34819fbf9e\n",
            "Successfully built nvidia-ml-py3\n",
            "Installing collected packages: nvidia-ml-py3, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, colab-dev-tools\n",
            "Successfully installed colab-dev-tools-0.0.10 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-ml-py3-7.352.0 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: dask[dataframe] in /usr/local/lib/python3.10/dist-packages (2023.8.1)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (2.2.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (24.0)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (1.4.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (6.0.1)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (7.1.0)\n",
            "Requirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (1.5.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask[dataframe]) (3.18.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->dask[dataframe]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->dask[dataframe]) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->dask[dataframe]) (1.25.2)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.2.0->dask[dataframe]) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.3->dask[dataframe]) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get flight data tar from my drive."
      ],
      "metadata": {
        "id": "XuDfJ71TXH5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown '1Ms9cVOIsf-Thr_ZDPLyjkXufQaB7NBJQ'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIfZf9X2ZHxH",
        "outputId": "906ddc2c-14e8-4c03-a209-945b977b904b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1Ms9cVOIsf-Thr_ZDPLyjkXufQaB7NBJQ\n",
            "From (redirected): https://drive.google.com/uc?id=1Ms9cVOIsf-Thr_ZDPLyjkXufQaB7NBJQ&confirm=t&uuid=73105003-a152-4025-b329-499e2f5c99fb\n",
            "To: /content/all_flight.tar.gz\n",
            "100% 4.29G/4.29G [01:03<00:00, 67.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "extract."
      ],
      "metadata": {
        "id": "UMjErKy8ZS9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "_ = tarfile.open('all_flight.tar.gz').extractall('')"
      ],
      "metadata": {
        "id": "8A-dgrxXZUs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone the causal jazz repo."
      ],
      "metadata": {
        "id": "VGcIJFJm-9wQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hugh-osborne/causaljazz.git"
      ],
      "metadata": {
        "id": "S7TK-FRF_Azx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1edd67da-ac13-4073-adf7-4dde9f500aa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'causaljazz'...\n",
            "remote: Enumerating objects: 905, done.\u001b[K\n",
            "remote: Counting objects: 100% (93/93), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 905 (delta 57), reused 63 (delta 29), pack-reused 812\u001b[K\n",
            "Receiving objects: 100% (905/905), 685.17 KiB | 2.80 MiB/s, done.\n",
            "Resolving deltas: 100% (474/474), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Move to the repo home directory."
      ],
      "metadata": {
        "id": "hWjRymWDgd8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('causaljazz')\n",
        "!git pull"
      ],
      "metadata": {
        "id": "Zx_KQLXggeX9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7594c1f-51a5-4765-e890-dd755e417190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manually install causal jazz. At some point we'll publish to pypi."
      ],
      "metadata": {
        "id": "uNwZu5sDgf2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install .\n",
        "os.chdir('..')"
      ],
      "metadata": {
        "id": "sxywh4gaghMt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6949837-ea9c-407b-f799-c10cae056f27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/causaljazz\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: causaljazz\n",
            "  Building wheel for causaljazz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for causaljazz: filename=causaljazz-0.0.1-py3-none-any.whl size=15806 sha256=9a65138e35105afd655cd2211f036150db7d27230b4563140ca23726ebbb23eb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-uu48o17w/wheels/b0/cd/89/3c771fb62baa20238264eb575d880299443d3de8a05d293808\n",
            "Successfully built causaljazz\n",
            "Installing collected packages: causaljazz\n",
            "Successfully installed causaljazz-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the example. Note, to begin with, set generate_models = True so that the model can be learned (later we can save the files.). Once the files have been generated, set it to false for repeat runs to save time."
      ],
      "metadata": {
        "id": "21vlZRcy_KiQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports."
      ],
      "metadata": {
        "id": "m0_Gk91eyBwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "from causaljazz.visualiser import Visualiser\n",
        "from causaljazz.inference import TEDAG\n",
        "from causaljazz.inference import TEDAG_FUNCTION\n",
        "from causaljazz.cpu import pmf\n",
        "from causaljazz.cpu import transition\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OecIvo2dyC9W",
        "outputId": "58d6d29c-c0b5-44a2-983c-9948718ebe11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pygame 2.5.2 (SDL 2.28.2, Python 3.10.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define some tensorflow model classes to be used later.\n"
      ],
      "metadata": {
        "id": "cUBopGg-vtee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class to learn a constant variance (with constant input val[0]) between the expected value val[1] and the predicted value val[2]\n",
        "class NoiseModel(tf.keras.Model):\n",
        "    def __init__(self, inputs, var_model, **kwargs):\n",
        "        super().__init__(inputs, **kwargs)\n",
        "        self.var_model = var_model\n",
        "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            vals = []\n",
        "            for d in data[0]:\n",
        "                vals = vals + [d]\n",
        "\n",
        "            var_model_z = self.var_model([vals[0]])\n",
        "            loss = tf.reduce_sum(tf.keras.losses.mean_squared_error((vals[2]-vals[1])*(vals[2]-vals[1]), var_model_z))\n",
        "\n",
        "            total_loss = loss\n",
        "\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "# Autoencoder class to learn throttle - possibly this can be generalised?\n",
        "class ThrottleModel(tf.keras.Model):\n",
        "    def __init__(self, inputs, throttle_encoder, throttle_decoder, **kwargs):\n",
        "        super().__init__(inputs, **kwargs)\n",
        "        self.throttle_encoder = throttle_encoder\n",
        "        self.throttle_decoder = throttle_decoder\n",
        "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            vals = []\n",
        "            for d in data[0]:\n",
        "                vals = vals + [d]\n",
        "\n",
        "            throttle_encoder_z = self.throttle_encoder([vals[1], vals[2], vals[0], vals[3]])\n",
        "            throttle_decoder_z = self.throttle_decoder([throttle_encoder_z, vals[1], vals[2], vals[3]])\n",
        "            loss = tf.reduce_sum(tf.keras.losses.mean_squared_error(vals[0], throttle_decoder_z))\n",
        "\n",
        "            total_loss = loss\n",
        "\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "# class to learn the timescale 'a' of a derivative where dx/dt = a*(stationary - x)\n",
        "# The timescale is constant and has the constant input 1 (vals[3])\n",
        "# vals[0] : predicted stationary value\n",
        "# vals[1] : current actual or predicted value\n",
        "# vals[2] : actual dx/dt (in our case the diff between two timepoints)\n",
        "# vals[3] : constant ones\n",
        "class DerivModel(tf.keras.Model):\n",
        "    def __init__(self, inputs, ts_model, **kwargs):\n",
        "        super().__init__(inputs, **kwargs)\n",
        "        self.ts_model = ts_model\n",
        "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            vals = []\n",
        "            for d in data[0]:\n",
        "                vals = vals + [d]\n",
        "\n",
        "            ts_model_z  = self.ts_model([vals[3]])\n",
        "            loss = tf.reduce_sum(tf.keras.losses.mean_absolute_error(tf.abs(vals[2]),(tf.multiply(ts_model_z,tf.abs(vals[0] - vals[1])))))\n",
        "\n",
        "            total_loss = loss\n",
        "\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "        }"
      ],
      "metadata": {
        "id": "oEE7Jsexv-iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now some helper functions..."
      ],
      "metadata": {
        "id": "XMmlppB5wnQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_pmf(sample, num_intervals = 100):\n",
        "  vmin = np.min(sample)\n",
        "  vmax = np.max(sample)\n",
        "\n",
        "  if vmax-vmin == 0:\n",
        "      vmin -= 0.005\n",
        "      vmax += 0.005\n",
        "\n",
        "  # Build a pmf for each latent variable\n",
        "  num_points = sample.shape[0]\n",
        "  mass_per_point = 1.0 / num_points\n",
        "  interval_width = (vmax-vmin)/num_intervals\n",
        "  pmf = np.zeros(num_intervals+1)\n",
        "  for i in sample:\n",
        "      idx = int((i - vmin) / interval_width)\n",
        "      pmf[idx] += mass_per_point\n",
        "\n",
        "  return pmf\n",
        "\n",
        "def build_cmf(pmf, num_intervals = 100):\n",
        "  # build cmfs for sampling\n",
        "  cmf = np.zeros(num_intervals+1)\n",
        "  mass = 0.0\n",
        "  for i in range(num_intervals+1):\n",
        "      mass += pmf[i]\n",
        "      cmf[i] = mass\n",
        "\n",
        "  return cmf\n",
        "\n",
        "def sample_from_cmf(cmf, num_points, vmin, vmax, num_intervals = 100):\n",
        "  interval_width = (vmax-vmin)/num_intervals\n",
        "\n",
        "  noise_rand = np.array([np.random.uniform() for a in range(num_points)])\n",
        "\n",
        "  noise_sampled = np.zeros(num_points)\n",
        "  for et in range(num_points):\n",
        "      for i in range(num_intervals+1):\n",
        "          if noise_rand[et] < cmf[i]:\n",
        "              noise_sampled[et] = vmin + (i * interval_width) + (interval_width * np.random.uniform())\n",
        "              break\n",
        "\n",
        "  return noise_sampled\n",
        "\n",
        "# get the data we care about for all flights for a given timestep\n",
        "def getFlightDataAtTimestep(dataset, current_timestep):\n",
        "  if current_timestep == 0:\n",
        "      current_timestep = 1\n",
        "\n",
        "  training_data = []\n",
        "\n",
        "  current_t = dataset[dataset['timestep'] == current_timestep]\n",
        "  current_t = current_t[['E1 RPM', 'E1 EGT1', 'E1 FFlow', 'VSpd', 'AltMSL', 'IAS', 'E1 OilP']]\n",
        "  previous_t = dataset[dataset['timestep'] == current_timestep-1]\n",
        "  previous_t = previous_t[['E1 RPM', 'E1 EGT1']]\n",
        "\n",
        "  # Loop through all flights up the max we want to view\n",
        "  flight_count = 0\n",
        "\n",
        "  for flight_index in [a for a in current_t.index]:\n",
        "      if flight_count > max_flights:\n",
        "          continue\n",
        "      flight_count += 1\n",
        "\n",
        "      if flight_index not in [a for a in current_t.index] or flight_index not in [a for a in previous_t.index]:\n",
        "          continue\n",
        "\n",
        "      vals_t = current_t.loc[flight_index].to_numpy().tolist()\n",
        "      vals_prev_t = previous_t.loc[flight_index].to_numpy().tolist()\n",
        "\n",
        "      # For the two variables we want, if either are NaN, just ignore this data point.\n",
        "      if (np.any([math.isnan(x) for x in vals_t]) or np.any([math.isnan(x) for x in vals_prev_t])):\n",
        "          continue\n",
        "\n",
        "      #                                  'E1 RPM', 'E1 EGT1', 'E1 FFlow',  'VSpd',   'AltMSL',   'IAS'             'dRPM',                       'dEGT'             'OilP'\n",
        "      training_data = training_data + [[vals_t[0], vals_t[1], vals_t[2], vals_t[3], vals_t[4], vals_t[5], vals_t[0] - vals_prev_t[0], vals_t[1] - vals_prev_t[1], vals_t[6]]]\n",
        "\n",
        "  return training_data\n",
        "\n",
        "def normaliseAndFormatData(data):\n",
        "  data = ((np.array(data)-min_vals) / max_vals) - 0.5\n",
        "  data_items = []\n",
        "  for d in range(data.shape[1]):\n",
        "      data_items = data_items + [np.reshape(data[:,d], [data[:,d].shape[0],1])]\n",
        "  return data_items\n",
        "\n",
        "# If we want to generate a pmf from a sample of points, we can use this function to get the desired cell widths\n",
        "def calculateCellGridMetrics(data, res):\n",
        "    in_mins = np.min(data, axis=1)\n",
        "    in_maxs = np.max(data, axis=1)\n",
        "    in_ranges = in_maxs - in_mins\n",
        "    in_ranges = np.array([r if r > 0 else 0.00001 for r in in_ranges]) # make sure no range is zero and give it a small epsilon range if so\n",
        "    return in_mins, in_maxs, in_ranges, np.divide(in_ranges,res)"
      ],
      "metadata": {
        "id": "l9EMaU9nwpmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now load the data. This takes some time but luckily only needs to be called once then later processing can be repeated - the magic of iPython!"
      ],
      "metadata": {
        "id": "4zjWPqKPwGvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "observed_variables = ['E1 RPM', 'IAS', 'E1 FFlow', 'E1 EGT1', 'AltMSL', 'VSpd', 'E1 OilP'] # In the future, add all four EGTs?\n",
        "\n",
        "# Define a maximum number of timesteps and flights to load\n",
        "max_flights = 500\n",
        "max_num_timesteps = 200\n",
        "\n",
        "# Load the flight header info\n",
        "flight_header_df = pd.read_csv('all_flights/flight_header.csv', index_col='Master Index')\n",
        "# Optionally search only for flights corresponding to a specific maintenance label\n",
        "flight_header_df = flight_header_df[flight_header_df['label'].str.contains('oil')]\n",
        "\n",
        "# Split flights into those before maintenance and after maintenance\n",
        "after_flights = flight_header_df[flight_header_df['number_flights_before'] == -1].index # -1 == flight after\n",
        "before_flights = flight_header_df[flight_header_df['number_flights_before'] == 0].index # >=0 == flight before\n",
        "\n",
        "# Load the flight data - to speed things up, only select the two (or more) variabels we care about olus timestep\n",
        "all_interested_columns = observed_variables + ['timestep']\n",
        "flight_data_ddf = dd.read_parquet('all_flights/one_parq', columns=all_interested_columns, index_col='Master Index')\n",
        "\n",
        "# Using the header data, split the flight data into before and after maintenance flights\n",
        "# Only read timesteps below the max required\n",
        "indices = [a for a in after_flights]\n",
        "flight_data_ddf_after = flight_data_ddf.loc[flight_data_ddf.index.isin(indices)]\n",
        "flight_data_ddf_after = flight_data_ddf_after.loc[flight_data_ddf_after['timestep'] < max_num_timesteps]\n",
        "flight_data_ddf_after = flight_data_ddf_after.loc[flight_data_ddf_after['AltMSL'] > 200] # avoid planes at ground level\n",
        "flight_data_df_after = flight_data_ddf_after.compute()\n",
        "\n",
        "indices = [a for a in before_flights]\n",
        "flight_data_ddf_before = flight_data_ddf.loc[flight_data_ddf.index.isin(indices)]\n",
        "flight_data_ddf_before = flight_data_ddf_before.loc[flight_data_ddf_before['timestep'] < max_num_timesteps]\n",
        "flight_data_ddf_before = flight_data_ddf_before.loc[flight_data_ddf_before['AltMSL'] > 200]\n",
        "flight_data_df_before = flight_data_ddf_before.compute()"
      ],
      "metadata": {
        "id": "mRw9H8OawR1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Format the data into a nice structure. See getFlightDataAtTimestep for the order of variables."
      ],
      "metadata": {
        "id": "BsCQkKqrxMzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data_after = []\n",
        "# load training data for flights after maintenance\n",
        "for current_timestep in [a+1 for a in range(max_num_timesteps-1)]:\n",
        "    training_data_after = training_data_after + getFlightDataAtTimestep(flight_data_df_after, current_timestep)\n",
        "\n",
        "min_vals = np.min(training_data_after, axis=0)\n",
        "max_vals  = np.max(training_data_after, axis=0)\n",
        "training_data_after = ((np.array(training_data_after)-min_vals) / max_vals) - 0.5\n",
        "\n",
        "training_data_before = []\n",
        "# load training data for flights before maintenance\n",
        "for current_timestep in [a+1 for a in range(max_num_timesteps-1)]:\n",
        "    training_data_before = training_data_before + getFlightDataAtTimestep(flight_data_df_before, current_timestep)\n",
        "\n",
        "training_data_before = ((np.array(training_data_before)-min_vals) / max_vals) - 0.5\n",
        "\n"
      ],
      "metadata": {
        "id": "Ao-KUGCOxS23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now define the different tensorflow models and train them if required."
      ],
      "metadata": {
        "id": "TTE2RxDsxsro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_models = True\n",
        "\n",
        "# Inputs must be separate because we're going to concatenate it\n",
        "rpm_input = tf.keras.Input(shape=(1))\n",
        "rpm_diff_input = tf.keras.Input(shape=(1))\n",
        "rpm_static_input = tf.keras.Input(shape=(1))\n",
        "fflow_input = tf.keras.Input(shape=(1))\n",
        "egt1_input = tf.keras.Input(shape=(1))\n",
        "egt1_diff_input = tf.keras.Input(shape=(1))\n",
        "egt1_static_input = tf.keras.Input(shape=(1))\n",
        "vspd_input = tf.keras.Input(shape=(1))\n",
        "constant_input = tf.keras.Input(shape=(1))\n",
        "ias_input = tf.keras.Input(shape=(1))\n",
        "alt_input = tf.keras.Input(shape=(1))\n",
        "oilp_input = tf.keras.Input(shape=(1))\n",
        "\n",
        "error = 100\n",
        "\n",
        "# This is the encoder part of an autoencoder to guess what throttle should be based on all inputs into fflow (and fflow itself)\n",
        "throttle_encoder_concat = layers.Concatenate(axis=1)([ias_input, alt_input, fflow_input, rpm_input])\n",
        "throttle_encoder_z = layers.Dense(200)(throttle_encoder_concat)\n",
        "throttle_encoder_z = layers.Dense(200)(throttle_encoder_z)\n",
        "throttle_encoder_z = layers.Dense(1)(throttle_encoder_z)\n",
        "throttle_encoder = tf.keras.Model([ias_input, alt_input, fflow_input, rpm_input], throttle_encoder_z, name=\"throttle_enc\")\n",
        "throttle_encoder.summary()\n",
        "\n",
        "# The decoder part of the throttle autoencoder is the function to calculate fflow\n",
        "throttle_decoder_concat = layers.Concatenate(axis=1)([throttle_encoder_z, ias_input, alt_input, rpm_input])\n",
        "throttle_decoder_z = layers.Dense(200)(throttle_decoder_concat)\n",
        "throttle_decoder_z = layers.Dense(200)(throttle_decoder_z)\n",
        "throttle_decoder_z = layers.Dense(1)(throttle_decoder_z)\n",
        "throttle_decoder = tf.keras.Model([throttle_encoder_z, ias_input, alt_input, rpm_input], throttle_decoder_z, name=\"throttle_dec\")\n",
        "throttle_decoder.summary()\n",
        "\n",
        "# This is the function to calculate the expected value of EGTStat. We will add noise to this later.\n",
        "egt_stat_concat = layers.Concatenate(axis=1)([fflow_input, rpm_input])\n",
        "egt_stat_z = layers.Dense(200)(egt_stat_concat)\n",
        "egt_stat_z = layers.Dense(200)(egt_stat_z)\n",
        "egt_stat_z = layers.Dense(1)(egt_stat_z)\n",
        "egt_stat = tf.keras.Model([fflow_input, rpm_input], egt_stat_z, name=\"egt_stat\")\n",
        "egt_stat.summary()\n",
        "\n",
        "# This is the timescale for the change in EGT (dEGTdt) it should be a constant and positive (hence the sigmoid activation)\n",
        "egt_ts_concat = layers.Concatenate(axis=1)([constant_input])\n",
        "egt_ts_z = layers.Dense(1, activation='sigmoid')(egt_ts_concat)\n",
        "egt_ts = tf.keras.Model([constant_input], egt_ts_z, name=\"egt_ts\")\n",
        "egt_ts.summary()\n",
        "\n",
        "# This is the variance of EGT from the predicted value of EGTStat\n",
        "egt_stat_var_concat = layers.Concatenate(axis=1)([constant_input])\n",
        "egt_stat_var_z = layers.Dense(1)(egt_stat_var_concat)\n",
        "egt_stat_var = tf.keras.Model([constant_input], egt_stat_var_z, name=\"egt_stat_var\")\n",
        "egt_stat_var.summary()\n",
        "\n",
        "# This is the function to estimate OilP (oil pressure) which is dependent on the RPM\n",
        "oilp_z = layers.Dense(20)(rpm_input)\n",
        "oilp_z = layers.Dense(1)(oilp_z)\n",
        "oilp = tf.keras.Model([rpm_input], oilp_z, name=\"oilp\")\n",
        "oilp.summary()\n",
        "\n",
        "# This is the variance of OilP from the predicted expected value of OilP\n",
        "oilp_var_concat = layers.Concatenate(axis=1)([constant_input])\n",
        "oilp_var_z = layers.Dense(1)(oilp_var_concat)\n",
        "oilp_var = tf.keras.Model([constant_input], oilp_var_z, name=\"oilp_var\")\n",
        "oilp_var.summary()\n",
        "\n",
        "\n",
        "rpm_input_data = np.reshape(training_data_after[:,0], [training_data_after[:,0].shape[0],1])\n",
        "egt_input_data = np.reshape(training_data_after[:,1], [training_data_after[:,1].shape[0],1])\n",
        "fflow_input_data = np.reshape(training_data_after[:,2], [training_data_after[:,2].shape[0],1])\n",
        "vspd_input_data = np.reshape(training_data_after[:,3], [training_data_after[:,3].shape[0],1])\n",
        "alt_input_data = np.reshape(training_data_after[:,4], [training_data_after[:,4].shape[0],1])\n",
        "ias_input_data = np.reshape(training_data_after[:,5], [training_data_after[:,5].shape[0],1])\n",
        "rpm_diff_input_data = np.reshape(training_data_after[:,6], [training_data_after[:,6].shape[0],1])\n",
        "egt1_diff_input_data = np.reshape(training_data_after[:,7], [training_data_after[:,7].shape[0],1])\n",
        "oilp_input_data = np.reshape(training_data_after[:,8], [training_data_after[:,8].shape[0],1])\n",
        "constant_ones = np.ones(training_data_after[:,0].shape)\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
        "                                            patience=20,\n",
        "                                            restore_best_weights=True)\n",
        "\n",
        "# Learn the latent variable Throttle and the function to calculate fflow\n",
        "throttle_model = ThrottleModel([fflow_input, ias_input, alt_input, rpm_input], throttle_encoder, throttle_decoder)\n",
        "if generate_models:\n",
        "    print(\"Throttle/FFlow Training...\")\n",
        "    throttle_model.compile(optimizer=tf.keras.optimizers.Adam())\n",
        "    throttle_model.fit([fflow_input_data,\n",
        "                     ias_input_data,\n",
        "                     alt_input_data,\n",
        "                     rpm_input_data],\n",
        "             epochs=200, batch_size=1000, verbose=1, callbacks=[callback])\n",
        "    throttle_model.save_weights('throttle_model.weights')\n",
        "else:\n",
        "    throttle_model.load_weights('throttle_model.weights')\n",
        "\n",
        "# Learn the OilP values\n",
        "\n",
        "if generate_models:\n",
        "    print(\"OilP Training...\")\n",
        "    oilp.compile(loss='mae', optimizer=tf.keras.optimizers.Adam())\n",
        "    oilp.fit([rpm_input_data], oilp_input_data, epochs=200, batch_size=1000, verbose=1, callbacks=[callback], validation_split=0.2)\n",
        "    oilp.save('oilp.model')\n",
        "else:\n",
        "    oilp = tf.keras.models.load_model('oilp.model')\n",
        "\n",
        "oilp_predicted = oilp.predict([rpm_input_data], verbose=0)\n",
        "\n",
        "# Learn the egt stationary values\n",
        "\n",
        "if generate_models:\n",
        "    print(\"EGT Stationary Training...\")\n",
        "    egt_stat.compile(loss='mae', optimizer=tf.keras.optimizers.Adam())\n",
        "    egt_stat.fit([fflow_input_data,rpm_input_data], egt_input_data, epochs=200, batch_size=1000, verbose=1, callbacks=[callback], validation_split=0.2)\n",
        "    egt_stat.save('egt_stat.model')\n",
        "else:\n",
        "    egt_stat = tf.keras.models.load_model('egt_stat.model')\n",
        "\n",
        "egt_stat_predicted = egt_stat.predict([fflow_input_data, rpm_input_data], verbose=0)\n",
        "\n",
        "# Learn the timescale of dEGTdt\n",
        "egt_deriv_model = DerivModel([egt1_static_input, egt1_input, egt1_diff_input,constant_input], egt_ts)\n",
        "if generate_models:\n",
        "    print(\"Timescale Training...\")\n",
        "    egt_deriv_model.compile(optimizer=tf.keras.optimizers.Adam())\n",
        "    egt_deriv_model.fit([egt_stat_predicted,\n",
        "                     egt_input_data,\n",
        "                     egt1_diff_input_data,\n",
        "                     constant_ones],\n",
        "             epochs=200, batch_size=1000, verbose=1, callbacks=[callback])\n",
        "    egt_deriv_model.save_weights('egt_ts_model.weights')\n",
        "else:\n",
        "    egt_deriv_model.load_weights('egt_ts_model.weights')\n",
        "\n",
        "# Learn the (assumed) constant variance of EGTStat\n",
        "egt_noise_model = NoiseModel([constant_input, egt_stat_z, egt1_input], egt_stat_var)\n",
        "if generate_models:\n",
        "    print(\"EGT Noise Training...\")\n",
        "    egt_noise_model.compile(optimizer=tf.keras.optimizers.Adam())\n",
        "    egt_noise_model.fit([constant_ones,\n",
        "                     egt_stat_predicted,\n",
        "                     egt_input_data],\n",
        "             epochs=200, batch_size=1000, verbose=1, callbacks=[callback])\n",
        "    egt_noise_model.save_weights('egt_noise_model.weights')\n",
        "else:\n",
        "    egt_noise_model.load_weights('egt_noise_model.weights')\n",
        "\n",
        "# Learn the (assumed) constant variance of OilP\n",
        "oilp_noise_model = NoiseModel([constant_input, oilp_z, oilp_input], oilp_var)\n",
        "if generate_models:\n",
        "    print(\"OilP Noise Training...\")\n",
        "    oilp_noise_model.compile(optimizer=tf.keras.optimizers.Adam())\n",
        "    oilp_noise_model.fit([constant_ones,\n",
        "                     oilp_predicted,\n",
        "                     oilp_input_data],\n",
        "             epochs=200, batch_size=1000, verbose=1, callbacks=[callback])\n",
        "    oilp_noise_model.save_weights('oilp_noise_model.weights')\n",
        "else:\n",
        "    oilp_noise_model.load_weights('oilp_noise_model.weights')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JE-t6F9-xyEP",
        "outputId": "0825963b-e180-41c0-cda9-c84387e3da27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"throttle_enc\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_10 (InputLayer)       [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " input_11 (InputLayer)       [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)        [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)        [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 4)                    0         ['input_10[0][0]',            \n",
            "                                                                     'input_11[0][0]',            \n",
            "                                                                     'input_4[0][0]',             \n",
            "                                                                     'input_1[0][0]']             \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 200)                  1000      ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 200)                  40200     ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 1)                    201       ['dense_1[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 41401 (161.72 KB)\n",
            "Trainable params: 41401 (161.72 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"throttle_dec\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_13 (InputLayer)       [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " input_10 (InputLayer)       [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " input_11 (InputLayer)       [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)        [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate  (None, 4)                    0         ['input_13[0][0]',            \n",
            " )                                                                   'input_10[0][0]',            \n",
            "                                                                     'input_11[0][0]',            \n",
            "                                                                     'input_1[0][0]']             \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 200)                  1000      ['concatenate_1[1][0]']       \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 200)                  40200     ['dense_3[1][0]']             \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 1)                    201       ['dense_4[1][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 41401 (161.72 KB)\n",
            "Trainable params: 41401 (161.72 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"egt_stat\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)        [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)        [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate  (None, 2)                    0         ['input_4[0][0]',             \n",
            " )                                                                   'input_1[0][0]']             \n",
            "                                                                                                  \n",
            " dense_6 (Dense)             (None, 200)                  600       ['concatenate_2[0][0]']       \n",
            "                                                                                                  \n",
            " dense_7 (Dense)             (None, 200)                  40200     ['dense_6[0][0]']             \n",
            "                                                                                                  \n",
            " dense_8 (Dense)             (None, 1)                    201       ['dense_7[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 41001 (160.16 KB)\n",
            "Trainable params: 41001 (160.16 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"egt_ts\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_9 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " concatenate_3 (Concatenate  (None, 1)                 0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 2         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2 (8.00 Byte)\n",
            "Trainable params: 2 (8.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Model: \"egt_stat_var\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_9 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " concatenate_4 (Concatenate  (None, 1)                 0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1)                 2         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2 (8.00 Byte)\n",
            "Trainable params: 2 (8.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Model: \"oilp\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 20)                40        \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 61 (244.00 Byte)\n",
            "Trainable params: 61 (244.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Model: \"oilp_var\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_9 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " concatenate_5 (Concatenate  (None, 1)                 0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 1)                 2         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2 (8.00 Byte)\n",
            "Trainable params: 2 (8.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Throttle/FFlow Training...\n",
            "Epoch 1/200\n",
            "57/57 [==============================] - 3s 22ms/step - loss: 21.7300\n",
            "Epoch 2/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 0.0806\n",
            "Epoch 3/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 0.0021\n",
            "Epoch 4/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 1.6267e-04\n",
            "Epoch 5/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 1.1916e-05\n",
            "Epoch 6/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 6.7326e-07\n",
            "Epoch 7/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 2.8600e-08\n",
            "Epoch 8/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 5.7040e-10\n",
            "Epoch 9/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 2.8693e-11\n",
            "Epoch 10/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 8.3334e-12\n",
            "Epoch 11/200\n",
            "57/57 [==============================] - 1s 14ms/step - loss: 4.0394e-12\n",
            "Epoch 12/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 2.7939e-12\n",
            "Epoch 13/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 2.0276e-12\n",
            "Epoch 14/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 1.7168e-12\n",
            "Epoch 15/200\n",
            "57/57 [==============================] - 1s 21ms/step - loss: 1.5722e-12\n",
            "Epoch 16/200\n",
            "57/57 [==============================] - 1s 22ms/step - loss: 1.4478e-12\n",
            "Epoch 17/200\n",
            "57/57 [==============================] - 1s 14ms/step - loss: 1.3760e-12\n",
            "Epoch 18/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 1.3222e-12\n",
            "Epoch 19/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 1.2931e-12\n",
            "Epoch 20/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 1.2492e-12\n",
            "Epoch 21/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 1.2500e-12\n",
            "Epoch 22/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 1.2281e-12\n",
            "Epoch 23/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 1.2382e-12\n",
            "Epoch 24/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 1.2202e-12\n",
            "Epoch 25/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 1.2440e-12\n",
            "Epoch 26/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 1.1999e-12\n",
            "Epoch 27/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 1.1943e-12\n",
            "Epoch 28/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 1.2117e-12\n",
            "Epoch 29/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 1.2028e-12\n",
            "Epoch 30/200\n",
            "57/57 [==============================] - 1s 19ms/step - loss: 1.2133e-12\n",
            "Epoch 31/200\n",
            "57/57 [==============================] - 1s 21ms/step - loss: 1.2910e-12\n",
            "Epoch 32/200\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 1.2374e-12\n",
            "Epoch 33/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 2.6292e-12\n",
            "Epoch 34/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 22.3512\n",
            "Epoch 35/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 3.7632\n",
            "Epoch 36/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 0.2814\n",
            "Epoch 37/200\n",
            "57/57 [==============================] - 1s 14ms/step - loss: 0.0202\n",
            "Epoch 38/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 0.0017\n",
            "Epoch 39/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 4.6274e-04\n",
            "Epoch 40/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 1.7706e-04\n",
            "Epoch 41/200\n",
            "57/57 [==============================] - 1s 14ms/step - loss: 6.8659e-05\n",
            "Epoch 42/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 2.1790e-05\n",
            "Epoch 43/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 8.0348e-06\n",
            "Epoch 44/200\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 2.3761e-06\n",
            "Epoch 45/200\n",
            "57/57 [==============================] - 1s 18ms/step - loss: 7.0473e-07\n",
            "Epoch 46/200\n",
            "57/57 [==============================] - 1s 21ms/step - loss: 1.8660e-07\n",
            "Epoch 47/200\n",
            "57/57 [==============================] - 1s 19ms/step - loss: 6.7973e-08\n",
            "OilP Training...\n",
            "Epoch 1/200\n",
            "46/46 [==============================] - 1s 6ms/step - loss: 0.1161 - val_loss: 0.0629\n",
            "Epoch 2/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0539 - val_loss: 0.0382\n",
            "Epoch 3/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.0389\n",
            "Epoch 4/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.0391\n",
            "Epoch 5/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0389\n",
            "Epoch 6/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.0389\n",
            "Epoch 7/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0390\n",
            "Epoch 8/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.0389\n",
            "Epoch 9/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0391\n",
            "Epoch 10/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.0389\n",
            "Epoch 11/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.0390\n",
            "Epoch 12/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0388\n",
            "Epoch 13/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0388\n",
            "Epoch 14/200\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.0431 - val_loss: 0.0390\n",
            "Epoch 15/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.0388\n",
            "Epoch 16/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0391\n",
            "Epoch 17/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0389\n",
            "Epoch 18/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0391\n",
            "Epoch 19/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0387\n",
            "Epoch 20/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.0393\n",
            "Epoch 21/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0388\n",
            "Epoch 22/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0388\n",
            "Epoch 23/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0389\n",
            "Epoch 24/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0389\n",
            "Epoch 25/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0391\n",
            "Epoch 26/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0389\n",
            "Epoch 27/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.0388\n",
            "Epoch 28/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0387\n",
            "Epoch 29/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0396\n",
            "Epoch 30/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0388\n",
            "Epoch 31/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0388\n",
            "Epoch 32/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0390\n",
            "Epoch 33/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0388\n",
            "Epoch 34/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0388\n",
            "Epoch 35/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0389\n",
            "Epoch 36/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0391\n",
            "Epoch 37/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0390\n",
            "Epoch 38/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0393\n",
            "Epoch 39/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0386\n",
            "Epoch 40/200\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.0392\n",
            "EGT Stationary Training...\n",
            "Epoch 1/200\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 0.0546 - val_loss: 0.0413\n",
            "Epoch 2/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.0437 - val_loss: 0.0416\n",
            "Epoch 3/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.0437 - val_loss: 0.0420\n",
            "Epoch 4/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.0439 - val_loss: 0.0417\n",
            "Epoch 5/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.0438 - val_loss: 0.0412\n",
            "Epoch 6/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.0445 - val_loss: 0.0435\n",
            "Epoch 7/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0438 - val_loss: 0.0412\n",
            "Epoch 8/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0443 - val_loss: 0.0439\n",
            "Epoch 9/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0440 - val_loss: 0.0441\n",
            "Epoch 10/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.0440 - val_loss: 0.0430\n",
            "Epoch 11/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0438 - val_loss: 0.0428\n",
            "Epoch 12/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0438 - val_loss: 0.0439\n",
            "Epoch 13/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.0439 - val_loss: 0.0430\n",
            "Epoch 14/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0445 - val_loss: 0.0411\n",
            "Epoch 15/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0438 - val_loss: 0.0418\n",
            "Epoch 16/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0437 - val_loss: 0.0417\n",
            "Epoch 17/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0440 - val_loss: 0.0444\n",
            "Epoch 18/200\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.0438 - val_loss: 0.0412\n",
            "Epoch 19/200\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 0.0440 - val_loss: 0.0413\n",
            "Epoch 20/200\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0439 - val_loss: 0.0421\n",
            "Epoch 21/200\n",
            "46/46 [==============================] - 1s 15ms/step - loss: 0.0438 - val_loss: 0.0424\n",
            "Epoch 22/200\n",
            "46/46 [==============================] - 1s 14ms/step - loss: 0.0437 - val_loss: 0.0433\n",
            "Epoch 23/200\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 0.0438 - val_loss: 0.0413\n",
            "Timescale Training...\n",
            "Epoch 1/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1071.8943\n",
            "Epoch 2/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1070.7311\n",
            "Epoch 3/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1069.8572\n",
            "Epoch 4/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1069.0071\n",
            "Epoch 5/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1067.7406\n",
            "Epoch 6/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1066.7675\n",
            "Epoch 7/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1066.1200\n",
            "Epoch 8/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1065.2951\n",
            "Epoch 9/200\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 1064.5375\n",
            "Epoch 10/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1064.3122\n",
            "Epoch 11/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1063.6094\n",
            "Epoch 12/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1062.7418\n",
            "Epoch 13/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1061.8850\n",
            "Epoch 14/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1061.9245\n",
            "Epoch 15/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1061.5128\n",
            "Epoch 16/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1061.0613\n",
            "Epoch 17/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1060.5068\n",
            "Epoch 18/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1060.3418\n",
            "Epoch 19/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1060.1649\n",
            "Epoch 20/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1060.3722\n",
            "Epoch 21/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1059.4194\n",
            "Epoch 22/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1059.4733\n",
            "Epoch 23/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1059.1975\n",
            "Epoch 24/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1058.8224\n",
            "Epoch 25/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1059.0028\n",
            "Epoch 26/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1059.0493\n",
            "Epoch 27/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1058.3210\n",
            "Epoch 28/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1058.5848\n",
            "Epoch 29/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1058.7452\n",
            "Epoch 30/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1057.7634\n",
            "Epoch 31/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1058.0285\n",
            "Epoch 32/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1058.1610\n",
            "Epoch 33/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1057.6053\n",
            "Epoch 34/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1057.2482\n",
            "Epoch 35/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1057.6218\n",
            "Epoch 36/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1057.5008\n",
            "Epoch 37/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1057.6366\n",
            "Epoch 38/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1057.5791\n",
            "Epoch 39/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1057.3000\n",
            "Epoch 40/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1057.4259\n",
            "Epoch 41/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1057.1921\n",
            "Epoch 42/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.8477\n",
            "Epoch 43/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1057.0824\n",
            "Epoch 44/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.9545\n",
            "Epoch 45/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1057.0940\n",
            "Epoch 46/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.8025\n",
            "Epoch 47/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.9672\n",
            "Epoch 48/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1057.1498\n",
            "Epoch 49/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.3698\n",
            "Epoch 50/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.2202\n",
            "Epoch 51/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.5123\n",
            "Epoch 52/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1057.1545\n",
            "Epoch 53/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1057.0690\n",
            "Epoch 54/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.2434\n",
            "Epoch 55/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.2332\n",
            "Epoch 56/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.6392\n",
            "Epoch 57/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.6184\n",
            "Epoch 58/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.7035\n",
            "Epoch 59/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.3328\n",
            "Epoch 60/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.6918\n",
            "Epoch 61/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.7470\n",
            "Epoch 62/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.3654\n",
            "Epoch 63/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.1345\n",
            "Epoch 64/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.9677\n",
            "Epoch 65/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.7857\n",
            "Epoch 66/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.5401\n",
            "Epoch 67/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.6128\n",
            "Epoch 68/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.0484\n",
            "Epoch 69/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.4679\n",
            "Epoch 70/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.5393\n",
            "Epoch 71/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.0567\n",
            "Epoch 72/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.2625\n",
            "Epoch 73/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.7082\n",
            "Epoch 74/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.3124\n",
            "Epoch 75/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.9891\n",
            "Epoch 76/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.4792\n",
            "Epoch 77/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.0435\n",
            "Epoch 78/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.8976\n",
            "Epoch 79/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.0370\n",
            "Epoch 80/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.5549\n",
            "Epoch 81/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.7612\n",
            "Epoch 82/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.1442\n",
            "Epoch 83/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.9828\n",
            "Epoch 84/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1056.1766\n",
            "Epoch 85/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.6285\n",
            "Epoch 86/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.5791\n",
            "Epoch 87/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.4839\n",
            "Epoch 88/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.3377\n",
            "Epoch 89/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.7393\n",
            "Epoch 90/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1056.3562\n",
            "Epoch 91/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.5018\n",
            "Epoch 92/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.9265\n",
            "Epoch 93/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.3927\n",
            "Epoch 94/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1056.3212\n",
            "Epoch 95/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1056.4197\n",
            "Epoch 96/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.3849\n",
            "Epoch 97/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.9087\n",
            "Epoch 98/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.6446\n",
            "Epoch 99/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.0487\n",
            "Epoch 100/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.2978\n",
            "Epoch 101/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.6703\n",
            "Epoch 102/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.6586\n",
            "Epoch 103/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.0818\n",
            "Epoch 104/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.9424\n",
            "Epoch 105/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.5340\n",
            "Epoch 106/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.7539\n",
            "Epoch 107/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.0792\n",
            "Epoch 108/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.7999\n",
            "Epoch 109/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1056.0178\n",
            "Epoch 110/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1056.1737\n",
            "Epoch 111/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 1055.7324\n",
            "Epoch 112/200\n",
            "57/57 [==============================] - 0s 5ms/step - loss: 1055.7741\n",
            "Epoch 113/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.7378\n",
            "Epoch 114/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.7549\n",
            "Epoch 115/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.5955\n",
            "Epoch 116/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1056.2109\n",
            "Epoch 117/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.4005\n",
            "Epoch 118/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.9858\n",
            "Epoch 119/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1056.1152\n",
            "Epoch 120/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.1167\n",
            "Epoch 121/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.7120\n",
            "Epoch 122/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.8453\n",
            "Epoch 123/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.5383\n",
            "Epoch 124/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.7319\n",
            "Epoch 125/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.4657\n",
            "Epoch 126/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.7699\n",
            "Epoch 127/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.5939\n",
            "Epoch 128/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.7347\n",
            "Epoch 129/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.7756\n",
            "Epoch 130/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.9193\n",
            "Epoch 131/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1056.0574\n",
            "Epoch 132/200\n",
            "57/57 [==============================] - 0s 5ms/step - loss: 1055.6915\n",
            "Epoch 133/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 1055.8178\n",
            "Epoch 134/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.7515\n",
            "Epoch 135/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 1055.4009\n",
            "Epoch 136/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.5637\n",
            "Epoch 137/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.9079\n",
            "Epoch 138/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 1055.6475\n",
            "Epoch 139/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 1055.9504\n",
            "Epoch 140/200\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 1056.1040\n",
            "Epoch 141/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.8368\n",
            "Epoch 142/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.2737\n",
            "Epoch 143/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.5496\n",
            "Epoch 144/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.5684\n",
            "Epoch 145/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.4365\n",
            "Epoch 146/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.0372\n",
            "Epoch 147/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.7234\n",
            "Epoch 148/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.1491\n",
            "Epoch 149/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.5112\n",
            "Epoch 150/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.8269\n",
            "Epoch 151/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.0277\n",
            "Epoch 152/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.7070\n",
            "Epoch 153/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.9968\n",
            "Epoch 154/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.7047\n",
            "Epoch 155/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.4757\n",
            "Epoch 156/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.6705\n",
            "Epoch 157/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.8194\n",
            "Epoch 158/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.6734\n",
            "Epoch 159/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.4673\n",
            "Epoch 160/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.6793\n",
            "Epoch 161/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.7267\n",
            "Epoch 162/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.9213\n",
            "Epoch 163/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.7868\n",
            "Epoch 164/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.5704\n",
            "Epoch 165/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.2287\n",
            "Epoch 166/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.5315\n",
            "Epoch 167/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.3799\n",
            "Epoch 168/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.3745\n",
            "Epoch 169/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.3354\n",
            "Epoch 170/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.4203\n",
            "Epoch 171/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.7675\n",
            "Epoch 172/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.4928\n",
            "Epoch 173/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.7919\n",
            "Epoch 174/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.5163\n",
            "Epoch 175/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1056.4691\n",
            "Epoch 176/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.6801\n",
            "Epoch 177/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.9629\n",
            "Epoch 178/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.5840\n",
            "Epoch 179/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.7855\n",
            "Epoch 180/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.8471\n",
            "Epoch 181/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.3474\n",
            "Epoch 182/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.8713\n",
            "Epoch 183/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.1555\n",
            "Epoch 184/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.7131\n",
            "Epoch 185/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.4390\n",
            "Epoch 186/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.5467\n",
            "Epoch 187/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.3107\n",
            "Epoch 188/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1055.7691\n",
            "Epoch 189/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 1056.0570\n",
            "Epoch 190/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.3930\n",
            "Epoch 191/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.0296\n",
            "Epoch 192/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.6022\n",
            "Epoch 193/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.3050\n",
            "Epoch 194/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.9335\n",
            "Epoch 195/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.0131\n",
            "Epoch 196/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.7494\n",
            "Epoch 197/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.5248\n",
            "Epoch 198/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.0540\n",
            "Epoch 199/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1055.1538\n",
            "Epoch 200/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1056.2502\n",
            "EGT Noise Training...\n",
            "Epoch 1/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 605.0597\n",
            "Epoch 2/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 446.8486\n",
            "Epoch 3/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 322.1407\n",
            "Epoch 4/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 225.9857\n",
            "Epoch 5/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 154.4091\n",
            "Epoch 6/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 102.3746\n",
            "Epoch 7/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 65.7768\n",
            "Epoch 8/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 40.8593\n",
            "Epoch 9/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 24.5854\n",
            "Epoch 10/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 14.3344\n",
            "Epoch 11/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 8.1346\n",
            "Epoch 12/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 4.5292\n",
            "Epoch 13/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 2.5857\n",
            "Epoch 14/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1.4687\n",
            "Epoch 15/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.9246\n",
            "Epoch 16/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.6478\n",
            "Epoch 17/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.5151\n",
            "Epoch 18/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4547\n",
            "Epoch 19/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4449\n",
            "Epoch 20/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4378\n",
            "Epoch 21/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4396\n",
            "Epoch 22/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4476\n",
            "Epoch 23/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4505\n",
            "Epoch 24/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4618\n",
            "Epoch 25/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4293\n",
            "Epoch 26/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4291\n",
            "Epoch 27/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4171\n",
            "Epoch 28/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4348\n",
            "Epoch 29/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4400\n",
            "Epoch 30/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4268\n",
            "Epoch 31/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4217\n",
            "Epoch 32/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4168\n",
            "Epoch 33/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4424\n",
            "Epoch 34/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4297\n",
            "Epoch 35/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4334\n",
            "Epoch 36/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4240\n",
            "Epoch 37/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4625\n",
            "Epoch 38/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4461\n",
            "Epoch 39/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4295\n",
            "Epoch 40/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4581\n",
            "Epoch 41/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4350\n",
            "Epoch 42/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4485\n",
            "Epoch 43/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4342\n",
            "Epoch 44/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4399\n",
            "Epoch 45/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4538\n",
            "Epoch 46/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4468\n",
            "Epoch 47/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4282\n",
            "Epoch 48/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4580\n",
            "Epoch 49/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4475\n",
            "Epoch 50/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4415\n",
            "Epoch 51/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4189\n",
            "Epoch 52/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4269\n",
            "Epoch 53/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4326\n",
            "Epoch 54/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4400\n",
            "Epoch 55/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4658\n",
            "Epoch 56/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4165\n",
            "Epoch 57/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4313\n",
            "Epoch 58/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4488\n",
            "Epoch 59/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4168\n",
            "Epoch 60/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4224\n",
            "Epoch 61/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4160\n",
            "Epoch 62/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4446\n",
            "Epoch 63/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.4505\n",
            "Epoch 64/200\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4367\n",
            "OilP Noise Training...\n",
            "Epoch 1/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1.0000\n",
            "Epoch 2/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.9680\n",
            "Epoch 3/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1.0022\n",
            "Epoch 4/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.9838\n",
            "Epoch 5/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.9780\n",
            "Epoch 6/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.9714\n",
            "Epoch 7/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.9728\n",
            "Epoch 8/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.9575\n",
            "Epoch 9/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.9293\n",
            "Epoch 10/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.9751\n",
            "Epoch 11/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.9851\n",
            "Epoch 12/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.9473\n",
            "Epoch 13/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.9555\n",
            "Epoch 14/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.9332\n",
            "Epoch 15/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.9586\n",
            "Epoch 16/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.9458\n",
            "Epoch 17/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.9558\n",
            "Epoch 18/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.9531\n",
            "Epoch 19/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.9328\n",
            "Epoch 20/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.9779\n",
            "Epoch 21/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.9822\n",
            "Epoch 22/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.9127\n",
            "Epoch 23/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.9696\n",
            "Epoch 24/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.9227\n",
            "Epoch 25/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.9520\n",
            "Epoch 26/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.9738\n",
            "Epoch 27/200\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.9829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to build the Causal Jazz simulation. First define the functions of the SCM."
      ],
      "metadata": {
        "id": "ZHzWfEPayo9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareDataForPrediction(y):\n",
        "    y = np.array(y)\n",
        "    prepared = []\n",
        "    for d in range(y.shape[1]):\n",
        "        data = y[:,d]\n",
        "        data = np.reshape(data, [data.shape[0],1])\n",
        "        prepared = prepared + [data]\n",
        "\n",
        "    return prepared\n",
        "\n",
        "# Estimate the constant timescales to be applied to dRPMdt and dEGTdt in rpm_func and egt_func below\n",
        "# dEGTdt timescale\n",
        "egt_ts_predicted = egt_deriv_model.ts_model.predict([constant_ones], verbose=0)\n",
        "egt_ts_constant = egt_ts_predicted[0,0]\n",
        "\n",
        "# This is our estimated SCM\n",
        "def fflow_func(y):\n",
        "    data = prepareDataForPrediction(y)\n",
        "    fflow = throttle_model.throttle_decoder.predict(data,verbose=0)\n",
        "    return np.transpose(np.array([fflow[:,0]]))\n",
        "\n",
        "def egt_stationary_func(y):\n",
        "    #data[0] is the noise and is added separately\n",
        "    data = prepareDataForPrediction(np.array(y)[:,1:])\n",
        "    noise_data = prepareDataForPrediction(y)\n",
        "    egt_stat_out = egt_stat.predict(data, verbose=0)\n",
        "    return np.transpose(np.array([np.array(egt_stat_out[:,0]) + np.array(noise_data)[0,:,0]]))\n",
        "\n",
        "def egt_func(y):\n",
        "    data = prepareDataForPrediction(y)\n",
        "    egt_prime = egt_ts_constant * (np.array(data)[1,:,0] - np.array(data)[0,:,0])\n",
        "    egt_new = np.array(data)[0,:,0] + egt_prime\n",
        "    return np.transpose(np.array([egt_new]))\n",
        "\n",
        "def oilp_func(y):\n",
        "    #data[0] is the noise and is added separately\n",
        "    data = prepareDataForPrediction(np.array(y)[:,1:])\n",
        "    noise_data = prepareDataForPrediction(y)\n",
        "    oilp_out = oilp.predict(data, verbose=0)\n",
        "    return np.transpose(np.array([np.array(oilp_out[:,0]) + np.array(noise_data)[0,:,0]]))\n",
        "\n",
        "trans_fflow_func = transition(fflow_func, 4)\n",
        "trans_egtstat_func = transition(egt_stationary_func, 3)\n",
        "trans_egt_func = transition(egt_func, 2)\n",
        "trans_oilp_func = transition(oilp_func, 2)"
      ],
      "metadata": {
        "id": "2TpCdXlKyw7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now build the initial distributions. res is the grid resolution of all discretised distributions. As most of the data are normalised, this amounts to a cell width of approximately 0.02."
      ],
      "metadata": {
        "id": "T9YLN_2WzptI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = 50\n",
        "\n",
        "# From the full data, predict the distribution of throttle and calculate the constant variance for EGTNoise, RPMNoise\n",
        "# egt_stat_predicted and rpm_stat_predicted have already been generated as they were used to learn RPMNoise and EGTNoise\n",
        "# dEGTdt timescale and dRPMdt timescale have already been calculated to be used in egt_func and rpm_func\n",
        "\n",
        "# Throttle\n",
        "throttle_predicted = throttle_model.throttle_encoder.predict([ias_input_data, alt_input_data, fflow_input_data, rpm_input_data],verbose=0)\n",
        "\n",
        "# EGTNoise\n",
        "egt_stat_var_predicted = egt_noise_model.var_model.predict([constant_ones],verbose=0) # Hopefully, noise_encoder has encoded a nice gaussianish variable (not guaranteed at all)\n",
        "egt_stat_var_constant = egt_stat_var_predicted[0,0]\n",
        "\n",
        "# OilPNoise\n",
        "oilp_var_predicted = oilp_noise_model.var_model.predict([constant_ones],verbose=0) # Hopefully, noise_encoder has encoded a nice gaussianish variable (not guaranteed at all)\n",
        "oilp_var_constant = oilp_var_predicted[0,0]\n",
        "\n",
        "# Number of points to deal with at a time\n",
        "num_points = 100\n",
        "\n",
        "# First, sample num_points values from the throttle distribution\n",
        "throttle_pmf = build_pmf(throttle_predicted[:,0])\n",
        "throttle_cmf = build_cmf(throttle_pmf)\n",
        "\n",
        "\n",
        "# Get the starting data for our observed variables\n",
        "data_items = normaliseAndFormatData(getFlightDataAtTimestep(flight_data_df_after, 1))\n",
        "red_ias_input_data = data_items[5][:,0]\n",
        "red_alt_input_data = data_items[4][:,0]\n",
        "red_rpm_input_data = data_items[0][:,0]\n",
        "red_egt_input_data = data_items[1][:,0]\n",
        "red_oilp_input_data = data_items[8][:,0]\n",
        "red_egt_stat_predicted = egt_stat_predicted[:,0]\n",
        "\n",
        "throttle_noise_basic_sampled = sample_from_cmf(throttle_cmf, red_ias_input_data.shape[0], np.min(throttle_predicted), np.max(throttle_predicted))\n",
        "\n",
        "# The resolution of our grids - right now, we only allow a single value for all dimensions (there's a bug when it's different)\n",
        "\n",
        "\n",
        "# Build the initial distributions based on the initial data\n",
        "throttle_noise_pmf = pmf(build_pmf(throttle_noise_basic_sampled,num_intervals=res), np.array([(np.max(throttle_noise_basic_sampled)-np.min(throttle_noise_basic_sampled))/res]), 0.00001)\n",
        "ias_pmf = pmf(build_pmf(red_ias_input_data,num_intervals=res), np.array([(np.max(red_ias_input_data)-np.min(red_ias_input_data))/res]), 0.00001)\n",
        "alt_pmf = pmf(build_pmf(red_alt_input_data,num_intervals=res),  np.array([(np.max(red_alt_input_data)-np.min(red_alt_input_data))/res]), 0.00001)\n",
        "rpm_input_pmf = pmf(build_pmf(red_rpm_input_data,num_intervals=res),np.array([(np.max(red_rpm_input_data)-np.min(red_rpm_input_data))/res]), 0.000001)\n",
        "egt_input_pmf = pmf(build_pmf(red_egt_input_data,num_intervals=res), np.array([(np.max(red_egt_input_data)-np.min(red_egt_input_data))/res]), 0.000001)\n",
        "oilp_input_pmf = pmf(build_pmf(red_oilp_input_data,num_intervals=res),  np.array([(np.max(red_oilp_input_data)-np.min(red_oilp_input_data))/res]), 0.00001)\n",
        "\n",
        "# Build the initial noise dis mributions\n",
        "egt_stat_noise_points = np.reshape(np.random.normal(loc=0.0,scale=np.sqrt(egt_stat_var_constant), size=100000), (100000))\n",
        "egt_stat_noise_pmf = pmf(np.array([]), np.array([(np.max(egt_stat_noise_points)-np.min(egt_stat_noise_points))/res]), 0.000001)\n",
        "egt_stat_noise_points = np.reshape(egt_stat_noise_points, (100000,1))\n",
        "egt_stat_noise_pmf.generateInitialDistribtionFromSample(egt_stat_noise_points)\n",
        "\n",
        "oilp_noise_points = np.reshape(np.random.normal(loc=0.0,scale=np.sqrt(oilp_var_constant), size=100000), (100000))\n",
        "oilp_noise_pmf = pmf(np.array([]), np.array([(np.max(oilp_noise_points)-np.min(oilp_noise_points))/res]), 0.00001)\n",
        "oilp_noise_points = np.reshape(oilp_noise_points, (100000,1))\n",
        "oilp_noise_pmf.generateInitialDistribtionFromSample(oilp_noise_points)\n",
        "\n",
        "# Build template distributions for the remaining variables\n",
        "fflow_pmf_template = pmf(np.array([1.0]), np.array([(np.max(fflow_input_data)-np.min(fflow_input_data))/res]), 0.000001)\n",
        "egt_stat_template = pmf(np.array([1.0]), np.array([(np.max(egt_stat_predicted[:,0])-np.min(egt_stat_predicted[:,0]))/res]), 0.000001)\n",
        "oilp_template = pmf(np.array([1.0]), np.array([(np.max(oilp_input_data[:,0])-np.min(oilp_input_data[:,0]))/res]), 0.000001)\n",
        "rpm_pmf_template = pmf(np.array([1.0]), np.array([(np.max(rpm_input_data)-np.min(rpm_input_data))/res]), 0.000001)\n",
        "\n",
        "# Starting distributions for egt and rpm\n",
        "egt_noise_points = np.reshape(np.random.normal(loc=-0.2,scale=0.05, size=100000), (100000))\n",
        "egt_noise_pmf = pmf(np.array([]), np.array([1.0/res]), 0.000001)\n",
        "egt_noise_points = np.reshape(egt_noise_points, (100000,1))\n",
        "egt_noise_pmf.generateInitialDistribtionFromSample(egt_noise_points)\n",
        "\n",
        "rpm_noise_points = np.reshape(np.random.normal(loc=0.2,scale=0.05, size=100000), (100000))\n",
        "rpm_noise_pmf = pmf(np.array([]), np.array([(np.max(rpm_noise_points)-np.min(rpm_noise_points))/res]), 0.000001)\n",
        "rpm_noise_points = np.reshape(rpm_noise_points, (100000,1))\n",
        "rpm_noise_pmf.generateInitialDistribtionFromSample(rpm_noise_points)\n",
        "\n",
        "# Build initial distribution of observed variables and throttle\n",
        "data_points = np.stack([throttle_noise_basic_sampled, red_alt_input_data, red_ias_input_data, red_rpm_input_data, red_egt_input_data, red_oilp_input_data])\n",
        "_, _, _, cell_widths = calculateCellGridMetrics(data_points, [res,res,res,res,res,res])\n",
        "start_pmf = pmf(np.array([]), cell_widths, _mass_epsilon=0.000001)\n",
        "in_data_points = np.transpose(data_points)\n",
        "start_pmf.generateInitialDistribtionFromSample(in_data_points)"
      ],
      "metadata": {
        "id": "FtxFC6Edz3UX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the TEDAG and its TEDAG_FUNCTIONS.\n",
        "Define any interventions. This should include interventions to set the initial distributions at time 0."
      ],
      "metadata": {
        "id": "y-Smbmf00N-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "tedag_func_throttle = TEDAG_FUNCTION(['Throttle'], 'Throttle', 1, None, throttle_noise_pmf)\n",
        "tedag_func_egt_noise = TEDAG_FUNCTION(['EGTNoise'], 'EGTNoise', 1, None, egt_stat_noise_pmf)\n",
        "tedag_func_oilp_noise = TEDAG_FUNCTION(['OilPNoise'], 'OilPNoise', 1, None, oilp_noise_pmf)\n",
        "\n",
        "# We're just ignoring changes in IAS, AltMSL, and RPM for now so treat them like constants\n",
        "tedag_func_ias = TEDAG_FUNCTION(['IAS'], 'IAS', 1, None, ias_pmf)\n",
        "tedag_func_altmsl = TEDAG_FUNCTION(['AltMSL'], 'AltMSL', 1, None, alt_pmf)\n",
        "tedag_func_rpm = TEDAG_FUNCTION(['E1 RPM'], 'E1 RPM', 1, None, rpm_pmf_template)\n",
        "\n",
        "# Functions which are calculated \"instantly\" : less than a single dt.\n",
        "tedag_func_fflow = TEDAG_FUNCTION(['Throttle', 'IAS', 'AltMSL', 'E1 RPM'], 'E1 FFlow', 0, trans_fflow_func, fflow_pmf_template)\n",
        "tedag_func_egt_stat = TEDAG_FUNCTION(['EGTNoise', 'E1 FFlow', 'E1 RPM'], 'EGTStat', 0, trans_egtstat_func, egt_stat_template)\n",
        "tedag_func_oilp = TEDAG_FUNCTION(['OilPNoise','E1 RPM'], 'E1 OilP', 0, trans_oilp_func, oilp_template)\n",
        "\n",
        "# Currently we don't store dEGTdt and just calculate it during tedag_func_egt\n",
        "# but if we wished, we could make it explicit which would be nice to see and also to intervene on\n",
        "#tedag_func_egt_dt = TEDAG_FUNCTION(['EGTStat', 'E1 EGT1'], 'dEGTdt', 0)\n",
        "\n",
        "# Functions which are calculated over multiples of dts\n",
        "tedag_func_egt = TEDAG_FUNCTION(['E1 EGT1', 'EGTStat'], 'E1 EGT1', 1, trans_egt_func, egt_input_pmf)\n",
        "\n",
        "\n",
        "tedag = TEDAG(1.0, [tedag_func_throttle, tedag_func_egt_noise, tedag_func_ias, tedag_func_rpm,\n",
        "                    tedag_func_altmsl, tedag_func_fflow, tedag_func_oilp, tedag_func_oilp_noise,\n",
        "                    tedag_func_egt_stat, tedag_func_egt], ['E1 RPM', 'E1 EGT1'])\n",
        "\n",
        "tedag.addIntervention(['Throttle','AltMSL','IAS', 'E1 RPM', 'E1 EGT1', 'E1 OilP'], 0, start_pmf)\n",
        "tedag.addIntervention(['EGTNoise'], 0, egt_stat_noise_pmf)\n",
        "tedag.addIntervention(['OilPNoise'], 0, oilp_noise_pmf)\n",
        "tedag.addIntervention(['E1 RPM'], 50, rpm_noise_pmf)"
      ],
      "metadata": {
        "id": "KHCmd8K10tDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flags to say what to simulation and plot."
      ],
      "metadata": {
        "id": "x5vwYztx1D_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_data_fixed = False\n",
        "plot_data_issue = True\n",
        "plot_cj = True"
      ],
      "metadata": {
        "id": "i-JM3i3g1HFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simulate! Causal sample points are plotted in blue, post-maintenance (fixed) data points in green, and pre-maintenance (issue) data points in red."
      ],
      "metadata": {
        "id": "VB40qraT1IXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if plot_data_issue:\n",
        "  current_timestep_df_before = flight_data_df_before[['E1 RPM', 'E1 EGT1','timestep']]\n",
        "if plot_data_fixed:\n",
        "  current_timestep_df_after = flight_data_df_after[['E1 RPM', 'E1 EGT1','timestep']]\n",
        "\n",
        "vis_key_inds = [0, 1]\n",
        "vis_val_max = [0.0, 0.0]\n",
        "vis_val_min = [0.0, 0.0]\n",
        "\n",
        "iteration = 0\n",
        "# Loop through the timesteps up to the maximum we require. Note all flights begin in the \"grounded\" mode\n",
        "# so due to our IAS filter above, we will intially see no points until flights begin taking off\n",
        "for current_timestep in range(max_num_timesteps):\n",
        "\n",
        "    fig = plt.figure()\n",
        "    plt.xlim([-0.5,0.5])\n",
        "    plt.ylim([-0.5,0.5])\n",
        "    plt.xlabel(\"E1 RPM\")\n",
        "    plt.ylabel(\"E1 EGT1\")\n",
        "\n",
        "    if plot_cj:\n",
        "      # tedag step\n",
        "      while tedag.findNextFunctionAndApply(current_timestep):\n",
        "          continue\n",
        "\n",
        "      tedag_pmf = tedag.getPmfForIteration(['E1 RPM', 'E1 EGT1'], current_timestep)\n",
        "      if tedag_pmf is not None:\n",
        "          sampled_points = np.array(tedag_pmf.pmf.sample(num_points))\n",
        "          node_indices = [[n.key for n in tedag_pmf.nodes].index(a+str(current_timestep)) for a in ['E1 RPM', 'E1 EGT1']]\n",
        "          plt.plot(np.array(sampled_points)[:,node_indices[0]], np.array(sampled_points)[:,node_indices[1]], 'bo')\n",
        "\n",
        "    # Loop through all flights up the max we want to view\n",
        "    flight_count = 0\n",
        "\n",
        "    if plot_data_fixed:\n",
        "        current_timestep_df_a = current_timestep_df_after[current_timestep_df_after['timestep'] == current_timestep]\n",
        "        points_x = []\n",
        "        points_y = []\n",
        "        for flight_index in [a for a in current_timestep_df_a.index]:\n",
        "            if flight_count > num_points:\n",
        "                continue\n",
        "            flight_count += 1\n",
        "\n",
        "            if flight_index not in [a for a in current_timestep_df_a.index]:\n",
        "                continue\n",
        "\n",
        "            flight_timestep_vals = current_timestep_df_a.loc[flight_index].to_numpy().tolist()\n",
        "\n",
        "            # For the two variables we want, if either are NaN, just ignore this data point.\n",
        "            if (math.isnan(flight_timestep_vals[vis_key_inds[0]]) or math.isnan(flight_timestep_vals[vis_key_inds[1]])):\n",
        "                continue\n",
        "\n",
        "            # Calculate the screen-space location of the data point\n",
        "            val_pos = [0.0,0.0]\n",
        "\n",
        "            #print(flight_timestep_vals[vis_key_inds[0]], flight_timestep_vals[vis_key_inds[1]])\n",
        "            val_pos[0] = -0.5 + ((flight_timestep_vals[vis_key_inds[0]] - min_vals[0]) / max_vals[0])\n",
        "            val_pos[1] = -0.5 + ((flight_timestep_vals[vis_key_inds[1]] - min_vals[1]) / max_vals[1])\n",
        "\n",
        "            points_x += [val_pos[0]]\n",
        "            points_y += [val_pos[1]]\n",
        "\n",
        "        plt.plot(points_x, points_y, 'gx')\n",
        "\n",
        "    flight_count = 0\n",
        "\n",
        "    if plot_data_issue:\n",
        "        current_timestep_df_b = current_timestep_df_before[current_timestep_df_before['timestep'] == current_timestep]\n",
        "        points_x = []\n",
        "        points_y = []\n",
        "        for flight_index in [a for a in current_timestep_df_b.index]:\n",
        "            if flight_count > num_points:\n",
        "                continue\n",
        "            flight_count += 1\n",
        "\n",
        "            if flight_index not in [a for a in current_timestep_df_b.index]:\n",
        "                continue\n",
        "\n",
        "            flight_timestep_vals = current_timestep_df_b.loc[flight_index].to_numpy().tolist()\n",
        "\n",
        "            # For the two variables we want, if either are NaN, just ignore this data point.\n",
        "            if (math.isnan(flight_timestep_vals[vis_key_inds[0]]) or math.isnan(flight_timestep_vals[vis_key_inds[1]])):\n",
        "                continue\n",
        "\n",
        "            # Calculate the screen-space location of the data point\n",
        "            val_pos = [0.0,0.0]\n",
        "\n",
        "            #print(flight_timestep_vals[vis_key_inds[0]], flight_timestep_vals[vis_key_inds[1]])\n",
        "            val_pos[0] = -0.5 + ((flight_timestep_vals[vis_key_inds[0]] - min_vals[0]) / max_vals[0])\n",
        "            val_pos[1] = -0.5 + ((flight_timestep_vals[vis_key_inds[1]] - min_vals[1]) / max_vals[1])\n",
        "\n",
        "            points_x += [val_pos[0]]\n",
        "            points_y += [val_pos[1]]\n",
        "\n",
        "        plt.plot(np.array(points_x), np.array(points_y), 'r+')\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q8C1FrbI1NRf",
        "outputId": "dde08728-a6e4-48bd-9017-1689e83b0daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-81105ef3d710>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mplot_cj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0;31m# tedag step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m       \u001b[0;32mwhile\u001b[0m \u001b[0mtedag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindNextFunctionAndApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_timestep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m           \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/causaljazz/inference.py\u001b[0m in \u001b[0;36mfindNextFunctionAndApply\u001b[0;34m(self, iteration)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplyFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_pmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpmf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_pmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpmf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;31m# Copy output_pmf to new pmf and dump the input pmf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/causaljazz/cpu.py\u001b[0m in \u001b[0;36mapplyFunction\u001b[0;34m(self, in_pmf, out_pmf)\u001b[0m\n\u001b[1;32m    391\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_pmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_output_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_output_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_pmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_pmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoord\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_pmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mmass_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}